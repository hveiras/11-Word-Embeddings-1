{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "\n",
    "Sources:\n",
    "https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/\n",
    "http://sebastianruder.com/word-embeddings-1/index.html\n",
    "\n",
    "\n",
    "## A Brief of History\n",
    "Extracted from https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/\n",
    "\n",
    "Word embeddings are a epresentational basis for downstream NLP tasks like text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis, and so on.  \n",
    "\n",
    "Word embedding seems to be the dominating term at the moment, I often prefer the term distributional semantic model (since the underlying semantic theory is called [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)).  \n",
    "\n",
    "There are also many other alternative terms in use, from the very general distributed representation to the more specific semantic vector space or simply word space.  \n",
    "\n",
    "Methods for using automatically generated contextual features were developed more or less simultaneously around 1990 in several different research areas. One of the most influential early models was Latent Semantic Analysis/Indexing (LSA/LSI), developed in the context of information retrieval, and the precursor of today’s topic models.  \n",
    "\n",
    "The most well-known of these are probably Self Organizing Maps (SOM) and Simple Recurrent Networks (SRN), of which the latter is the precursor to today’s neural language models.\n",
    "\n",
    "Later developments are basically only refinements of these early models. Topic models are refinements of LSA, and include methods like probabilistic LSA (PLSA) and Latent Dirichlet Allocation (LDA).  \n",
    "\n",
    "The main difference between these various models is the type of contextual information they use. LSA and topic models use documents as contexts, which is a legacy from their roots in information retrieval. Neural language models and distributional semantic models instead use words as contexts, which is arguably more natural from a linguistic and cognitive perspective. These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”).  \n",
    "\n",
    "There is no qualitative difference between (current) predictive neural network models and count-based distributional semantics models. Rather, they are different computational means to arrive at the same type of semantic model; several recent papers have demonstrated both theoretically and empirically the correspondence between these different types of models. ([Österlund et al. (2015)](http://aclweb.org/anthology/D/D15/D15-1024.pdf), [Levy and Goldberg (2014)](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf))\n",
    "\n",
    "A good bet is to use a factorized model – either using explicit factorization of a distributional semantic model (available in e.g. the PyDSM python library or the GloVe implementation), or using a neural network model like those implemented in word2vec – since they produce state of the art results and are robust across a wide range of semantic tasks ([Schnabel et al., 2015](http://aclweb.org/anthology/D/D15/D15-1036.pdf)).\n",
    "\n",
    "Word embeddings are one of the few currently successful applications of unsupervised learning. Their main benefit arguably is that they don't require expensive annotation, but can be derived from large unannotated corpora that are readily available. Pre-trained embeddings can then be used in downstream tasks that use small amounts of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding models\n",
    "\n",
    "Every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as Embedding Layer.\n",
    "\n",
    "We assume the following notational standards: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3,⋯,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v′_w$ (another word representation whose role will soon become clearer). We finally optimize an objective function $J_θ$ with regard to our model parameters $θ$ and our model outputs some score $f_θ(x)$ for every input $x$.  \n",
    "\n",
    "### A note in language modelling\n",
    "\n",
    "Language models generally try to compute the probability of a word $w_t$ given its $n−1$ previous words, i.e. $p(wt|w_{t−1},⋯w_{t−n+1})$. By applying the chain rule together with the Markov assumption, we can approximate the probability of a whole sentence or document by the product of the probabilities of each word given its n previous words:\n",
    "\n",
    "$$p(w_1,⋯,w_T)=\\prod_i p(w_i|w_{i−1},⋯,w_{i−n+1})$$\n",
    "\n",
    "In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams:  \n",
    "\n",
    "$$p(w_t|w_{t−1},⋯,w_{t−n+1})=\\frac{count(w_{t−n+1},⋯,w_{t−1},w_t)}{count(w_{t−n+1},⋯,w_{t−1})}$$\n",
    "\n",
    "More info about n-grams in these [slides](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf) from Stanford.\n",
    "\n",
    "In neural networks, we achieve the same objective using the well-known softmax layer:  \n",
    "\n",
    "$$p(w_t|w_{t−1},⋯,w_{t−n+1})=\\frac{\\exp(h^⊤v′_{w_t})}{\\sum_{w_i∈V}exp(h^⊤v′_{w_i})}$$\n",
    "\n",
    "The inner product $h^⊤v′_{w_t}$ computes the (unnormalized) log-probability of word $w_t$, which we normalize by the sum of the log-probabilities of all words in $V$. $h$ is the output vector of the penultimate network layer (the hidden layer in the feed-forward network in Figure 1), while $v′_w$ is the output embedding of word $w$, i.e. its representation in the weight matrix of the softmax layer. Note that even though $v′_w$ represents the word $w$, it is learned separately from the input word embedding $v_w$, as the multiplications both vectors are involved in differ ($v_w$ is multiplied with an index vector, $v′_w$ with $h$).\n",
    "\n",
    "<figure text-align=center>\n",
    "  <img src=\"nn_language_model-1.jpg\">\n",
    "</figure>  \n",
    "\n",
    "Figure 1: A neural language model (Bengio et al., 2006)\n",
    "\n",
    "Note that we need to calculate the probability of every word $w$ at the output layer of the neural network. To do this efficiently, we perform a matrix multiplication between $h$ and a weight matrix whose rows consist of $v′_w$ of all words $w$ in $V$. We then feed the resulting vector, which is often referred to as a logit, i.e. the output of a previous layer that is not a probability, with $d=|V|$ into the softmax, while the softmax layer \"squashes\" the vector to a probability distribution over the words in $V$.\n",
    "\n",
    "In the state of the art algorithms the fully connected layer is replaced by an LSTM layer.\n",
    "\n",
    "Using this softmax layer, the model tries to maximize the probability of predicting the correct word at every timestep t. The whole model thus tries to maximize the averaged log probability of the whole corpus:\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^T log p(w_t|w_{t−1},⋯,w_{t−n+1})$$\n",
    "\n",
    "To sample words from the language model at test time, we can either greedily choose the word with the highest probability $p(w_t|w_{t−1}⋯w_{t−n+1})$ at every time step t or use beam search. We can do this for instance to generate arbitrary text sequences or as part of a sequence prediction task, where an LSTM is used as the decoder.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "### Continuous bag-of-words (CBOW)\n",
    "\n",
    "While a language model is only able to look at the past words for its predictions, as it is evaluated on its ability to predict each next word in the corpus, a model that just aims to generate accurate word embeddings does not suffer from this restriction. Mikolov et al. thus use both the n words before and after the target word wt to predict it as depicted in Figure 4. They call this continuous bag-of-words (CBOW), as it uses continuous representations whose order is of no importance.\n",
    "\n",
    "<figure text-align=center>\n",
    "    <img src=\"cbow.png\">\n",
    "</figure>\n",
    "Figure 4: Continuous bag-of-words (Mikolov et al., 2013)\n",
    "\n",
    "The objective function of CBOW in turn is only slightly different than the language model one:\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^T \\log p(w_t|w_{t−n},⋯,w_{t−1},w_{t+1},⋯,w_{t+n})$$\n",
    "\n",
    "Instead of feeding $n$ previous words into the model, the model receives a window of $n$ words around the target word $w_t$ at each time step $t$.\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "While CBOW can be seen as a precognitive language model, skip-gram turns the language model objective on its head: Instead of using the surrounding words to predict the centre word as with CBOW, skip-gram uses the centre word to predict the surrounding words as can be seen in Figure 5.\n",
    "\n",
    "<figure text-align=center>\n",
    "    <img src=\"skip-gram.png\">\n",
    "</figure>  \n",
    "Figure 5: Skip-gram (Mikolov et al., 2013)\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{−n≤j≤n,j≠0} \\log p(w_{t+j}|{w_t})$$\n",
    "\n",
    "This is done using the following softmax as a probability model:\n",
    "\n",
    "$$p(w_{t+j}|w_t)=\\frac{exp(v^⊤_{w_t} v′_{w_{t+j}})}{\\sum_{w_i∈V}exp(v^⊤_{w_t}v′_{w_i})}$$\n",
    "\n",
    "Note that $v_{w_i}$ is the embedding of word i, thus a column of the embedding matrix. $V'$ matrix is the matrix to decode the embedding representation into the logits vector. From the logits vector we can calculate the co-occurrence probability.\n",
    "\n",
    "The notation in Mikolov's paper differs slightly from ours, as they denote the centre word with $w_I$ and the surrounding words with $w_O$. If we replace $w_t$ with $w_I, w_{t+j}$ with $w_O$, and swap the vectors in the inner product due to its commutativity, we arrive at the softmax notation in their paper:\n",
    "\n",
    "$$p(w_O|w_I)=\\frac{\\exp(v′^⊤_{w_O}v_{w_I})}{\\sum^V_{w=1}\\exp(v′^⊤_{w_v}v_{w_I})}$$\n",
    "\n",
    "To train this softmax function is extremely computational expensive. For each training iteration the learning gradient must propagate not only through the desired target word output, but the al other outputs. This is computational expensive and the next Word2Vect papers are related to training optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The softmax\n",
    "\n",
    "Now new are going to take a deeper look at the softmax. For more insight please refer to [this link](http://cs231n.github.io/linear-classify/#softmax-classifier).  \n",
    "In the Softmax classifier, the function mapping $f(x_i;W)=W x_i$ represents the unnormalized log probabilities for each class. The asociated loss is a cross-entropy loss that has the form:\n",
    "\n",
    "$$L_i=-\\log \\left( \\frac{\\exp(f_{y_i})}{\\sum_j \\exp(f_j)} \\right) $$ or equivalently $$L_i=-f_{y_i}+\\log \\sum_j \\exp(f_j) $$\n",
    "\n",
    "where we are using the notation $f_j$ to mean the j-th element of the vector of class scores $f$. As before, the full loss for the dataset is the mean of $L_i$ over all training examples together with a regularization term $R(W)$. The function $f_j(z)=\\frac {\\exp(z_j)}{\\sum_k{\\exp(z_k)}}$ is called the softmax function: It takes a vector of arbitrary real-valued scores (in $z$) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate.\n",
    "\n",
    "### Information theory view. \n",
    "\n",
    "The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as:\n",
    "\n",
    "$$H(p,q)=−\\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ($q=\\frac{\\exp(f_{y_i}}{\\sum_j \\exp(f_j)}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p=[0,…1,…,0]$ contains a single $1$ at the $y_i$ -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q)=H(p)+DKL(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.\n",
    "\n",
    "### Probabilistic interpretation. \n",
    "\n",
    "Looking at the expression, we see that\n",
    "\n",
    "$$ P(y_i∣x_i;W)=\\frac{\\exp(f_{y_i})}{\\sum_j \\exp (f_j)} $$  \n",
    "\n",
    "can be interpreted as the (normalized) probability assigned to the correct label $y_i$ given the image $x_i$ and parameterized by $W$. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix $W$, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class.\n",
    "\n",
    "### Practical issues: Numeric stability\n",
    "\n",
    "When you’re writing code for computing the Softmax function in practice, the intermediate terms $\\exp(f_{y_i})$ and $\\sum_j(f_j)$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant $C$ and push it into the sum, we get the following (mathematically equivalent) expression:\n",
    "\n",
    "$$\\frac{\\exp(f_{y_i})}{\\sum_j{\\exp(f_j)}}=\\frac{C\\exp(f_{y_i})}{C\\sum_j{\\exp(f_j)}}=\\frac{\\exp(f_{y_i}+\\log C)}{\\sum_j{\\exp(f_j + \\log C)}}$$  \n",
    "We are free to choose the value of $C$. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for $C$ is to set $\\log C=−\\max_j(f_j)$. This simply states that we should shift the values inside the vector $f$ so that the highest value is zero. In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 456 789]\n",
      "[ 0.  0. nan]\n",
      "[-666 -333    0]\n",
      "[5.75274406e-290 2.39848787e-145 1.00000000e+000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cselmo/.conda/envs/doctorado-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "print(f)\n",
    "print(p)\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n",
    "print(f)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximating the softmax\n",
    "\n",
    "In this section we are going to take a look at the softmax function. If you want more detail about the softmax, please refer to [this link\n",
    "http://cs231n.github.io/linear-classify/#softmax-classifier\n",
    "\n",
    "We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) ([Jean et al.](http://www.aclweb.org/anthology/P15-1001)) and language modelling ([Jozefowicz et al.](https://arxiv.org/pdf/1602.02410.pdf)).\n",
    "\n",
    "There are several strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.\n",
    "\n",
    "We will explore Noise Contrastive Estimation, a Sampling-based approach to approximate the softmax. Then we will take a look to Negative Sampling. \n",
    "\n",
    "### Noise Contrastive Estimation\n",
    "\n",
    "Noise Contrastive Estimation (NCE) ([Gutmann and Hyvärinen](http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf)) is proposed by [Mnih and Teh](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf).\n",
    "In this case, we train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.\n",
    "\n",
    "<img src=\"negative_sampling.png\">\n",
    "\n",
    "Figure 4: Noise Contrastive Estimation ([Stephan Gouws' PhD dissertation](https://pdfs.semanticscholar.org/presentation/f775/cea7cd7f368094a853ea396121950d37915c.pdf))\n",
    "\n",
    "For every word $w_i$ given its context ci of n previous words $w_{t−1},⋯,w_{t−n+1}$ in the training set, we thus generate $k$ noise samples $\\tilde{w}_{ik}$ from a noise distribution $Q$. We can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\\tilde{w}_{ik}$ as false ($y=0$).  \n",
    "\n",
    "We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the positive log-likelihood as some papers do):\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[ \\log P(y=1|w_i,c_i)+ k \\mathbb{E}_{\\tilde{w}_{ik}\\sim Q} \\left[ \\log P(y=0|\\tilde{w}_{ij},c_i) \\right] \\right]$$\n",
    "\n",
    "Instead of computing the expectation $\\mathbb{E}_{\\tilde{w}_{ik}}∼Q$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log P(y=1|w_i,c_i) + k \\sum^k_{j=1}\\frac{1}{k} \\log P(y=0|\\tilde{w}_{ij},c_i)\\right]$$\n",
    "\n",
    "Which reduces to:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log P(y=1|w_i,c_i) +  \\sum^k_{j=1} \\log P(y=0|\\tilde{w}_{ij},c_i)\\right]$$\n",
    "\n",
    "By generating $k$ noise samples for every genuine word wi given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{train}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:\n",
    "\n",
    "$$P(y,w|c)=\\frac{1}{k+1}P_{train}(w|c)+\\frac{k}{k+1}Q(w)$$\n",
    "\n",
    "Given this mixture, we can now calculate the probability that a sample came from the training Ptrain distribution as a conditional probability of y given $w$ and $c$:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\frac{1}{k+1}P_{train}(w|c)}{\\frac{1}{k+1}P_{train}(w|c)+\\frac{k}{k+1}Q(w)}$$\n",
    "\n",
    "which can be simplified to:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{P_{train}(w|c)}{P_{train}(w|c)+k Q(w)}$$\n",
    "\n",
    "As we don't know $P_train$ (which is what we would like to calculate), we replace $P_train$ with the probability of our model $P$:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{P(w|c)}{P(w|c)+k Q(w)}$$\n",
    "\n",
    "The probability of predicting a noise sample $(y=0)$ is then simply $P(y=0|w,c)=1−P(y=1|w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word w given its context $c$ is essentially the definition of our softmax:\n",
    "\n",
    "$$P(w|c)=\\frac{\\exp h^T v'_w}{\\sum_{w_i∈V}\\exp(h^⊤ v′_{w_i})}$$\n",
    "\n",
    "For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:\n",
    "\n",
    "$$P(w|c)=\\frac{\\exp h^T v'_w}{Z(c)}$$\n",
    "\n",
    "Having to compute $P(w|c)$ means that -- again -- we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn. \n",
    "Mnih and Teh (2012) and [Vaswani et al.](http://www.aclweb.org/anthology/D13-1140) actually keep $Z(c)$ fixed at 1, which they report does not affect the model's performance. This assumption has the nice side-effect of reducing the model's parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, [Zoph et al.](http://www.aclweb.org/anthology/N16-1145.pdf) find that even when learned, $Z(c)$ is close to 1 and has low variance.\n",
    "\n",
    "If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:\n",
    "\n",
    "$$P(w|c)=exp(h^⊤ v′_w)$$\n",
    "\n",
    "Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log\\frac{\\exp(h^⊤ v′_{w_i})}{\\exp(h^⊤ v′_{w_i})+k Q(w_i)}+\\sum^k_{j=1} \\log(1−\\frac{\\exp(h^⊤ v′_{\\tilde{w}_{ij}})}{exp(h^⊤ v′_{\\tilde{w}_{ij}})+k Q(\\tilde{w}_{ij}))}\\right]$$\n",
    "\n",
    "Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples k, the NCE derivative tends towards the gradient of the softmax function. [Mnih and Teh (2012)](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf) report that 25 noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about 45. For more information on NCE, Chris Dyer has published some excellent [notes](https://arxiv.org/pdf/1410.8251.pdf).\n",
    "\n",
    "One caveat of NCE is that as typically different noise samples are sampled for every training word w, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Negative Sampling (NEG), the objective that has been popularised by [Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples $k$ increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.\n",
    "\n",
    "NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{train}$ given a context $c$ as follows:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\exp(h^⊤ v′_w)}{\\exp(h^⊤ v′_w)+kQ(w)}$$\n",
    "\n",
    "The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, kQ(w) to 1, which leaves us with:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\exp(h⊤v′w)}{\\exp(h⊤v′w)+1}$$\n",
    "\n",
    "kQ(w)=1 is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{1}{1+exp(−h^⊤ v′_w)}$$\n",
    "\n",
    "If we now insert this back into the logistic regression loss from before, we get:\n",
    "\n",
    "$$J_θ = −\\sum_{w_i∈V}\\left[ \\log{\\frac{1}{1+\\exp(−h^⊤ v′_{w_i})}} + \\sum^k_{j=1} \\log\\left(1-\\frac{1}{1+\\exp{ (-h^T v'_{w_{ij}}) }}\\right)\\right]$$\n",
    "\n",
    "By simplifying slightly, we obtain:\n",
    "\n",
    "$$J_θ = −\\sum_{w_i∈V}\\left[ \\log{\\frac{1}{1+\\exp(−h^⊤ v′_{w_i})}} + \\sum^k_{j=1} \\log \\frac{1}{1+\\exp{ (h^T v'_{w_{ij}}) }}\\right]$$\n",
    "\n",
    "\n",
    "Setting $σ(x)=\\frac{1}{1+\\exp(−x)} $finally yields the NEG loss:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log σ(h^⊤ v′_{w_i})+\\sum^k_{j=1} \\log σ(−h^⊤ v′_{\\tilde{w}_{ij}})\\right]$$\n",
    "\n",
    "To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_I}$, $v′_{w_i}$ with $v′_{w_O}$ and $v_{\\tilde{w}_{ij}}$ with $v′_{w_i}$. Also, in contrast to Mikolov's NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation $\\mathbb{E}[\\tilde{w}_{ik} \\sim Q]$ with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at [Goldberg and Levy's notes](http://arxiv.org/abs/1402.3722).\n",
    "\n",
    "We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Keras\n",
    "\n",
    "In this section we are going to implement Word2Vec using Keras for the New York Times articles database from 2010.  \n",
    "For this task, we must generate the positive and the negative samples.\n",
    "\n",
    "### Positive Samples\n",
    "\n",
    "The positive samples are the samples took from the empirical distribution $P_{train}$. The articles were pre-processed. Let's see an example of a pre-processed article from 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_id': '4fd399ea8eb7c8105d8d6889',\n",
       " 'type_of_material': 'Blog',\n",
       " 'blog': [],\n",
       " 'news_desk': None,\n",
       " 'lead_paragraph': 'Hiroyuki Ito, a freelancer for The New York Times, was born in Tokyo in 1968. He moved to New York 18 years ago, studied at the Tisch School of the Arts at New York University and began his freelancing career at The Village Voice. He now lives in Astoria, Queens. In \"Camera Obscura,\" Mr. Ito described his practice of storing exposed film rolls in the vegetable compartment of his refrigerator. \"Every few months, I need to develop all these films so that the vegetables can reclaim their space.\"',\n",
       " 'headline': {'main': 'New York, New York, New York, New York',\n",
       "  'kicker': 'Lens'},\n",
       " 'abstract': 'Inspired by a multiscreen video peep show in Times Square, Hiroyuki Ito set out to portray New York City in a seemingly random grid of pictures.',\n",
       " 'print_page': None,\n",
       " 'word_count': 287,\n",
       " 'snippet': 'Inspired by a multiscreen video peep show in Times Square, Hiroyuki Ito set out to portray New York City in a seemingly random grid of pictures....',\n",
       " 'source': 'The New York Times',\n",
       " 'slideshow_credits': None,\n",
       " 'web_url': 'https://lens.blogs.nytimes.com/2010/04/26/showcase-153/',\n",
       " 'multimedia': [],\n",
       " 'document_type': 'blogpost',\n",
       " 'subsection_name': None,\n",
       " 'keywords': [{'value': 'News', 'name': 'type_of_material', 'rank': '1'}],\n",
       " 'byline': {'person': [{'organization': '',\n",
       "    'role': 'reported',\n",
       "    'rank': 1,\n",
       "    'firstname': 'Hiroyuki',\n",
       "    'lastname': 'ITO'}],\n",
       "  'original': 'By HIROYUKI ITO'},\n",
       " 'pub_date': datetime.datetime(2010, 4, 26, 0, 0, 49),\n",
       " 'section_name': 'Multimedia',\n",
       " 'body': 'Hiroyuki Ito, a freelancer for The New York Times, was born in Tokyo in 1968. He moved to New York 18 years ago, studied at the Tisch School of the Arts at New York University and began his freelancing career at The Village Voice. He now lives in Astoria, Queens. In “Camera Obscura,” Mr. Ito described his practice of storing exposed film rolls in the vegetable compartment of his refrigerator. “Every few months, I need to develop all these films so that the vegetables can reclaim their space.”\\n\\n\\n\\nMore Photos Taking Pictures by Habit Hiroyuki Ito’s street photography was featured in a slide show in January 2009. Slide Show: “A Clueless Spectator”\\n\\n“Transfer of Guilt” is a collection of grids. Each grid contains four snapshots that were taken in New York from 2006 to 2009. They were all shot in black-and-white film, developed and printed on resin-coated paper before being scanned and assembled into grids.\\n\\nThe idea of making grids came from visiting a video booth in Times Square, where the viewer watches four porn movies simultaneously on a split screen.\\n\\nYou’re supposed to touch part of the screen to indicate which one you want to watch, then the split screen turns back to a normal full screen that plays the movie of your choice. I didn’t choose one over the others because I was afraid of touching the screen, which had fingerprint smudges all over from past visitors.\\n\\nLooking at four sad human dramas unfolding in front of my eyes was at least intellectually stimulating. Upon closer inspection, the random movies started to create rhythm of their own both visually and emotionally, as if John Cage was at work behind the screen.\\n\\nSince my working method in street photography is purely accidental, shuffling images into grids underlines their arbitrariness.\\n\\nI only supply clues, hoping that each viewer comes up with his or her own detective novel.',\n",
       " 'body_norm': [['hiroyuki',\n",
       "   'ito',\n",
       "   'a',\n",
       "   'freelancer',\n",
       "   'for',\n",
       "   'the',\n",
       "   'new',\n",
       "   'york',\n",
       "   'times',\n",
       "   'be',\n",
       "   'bear',\n",
       "   'in',\n",
       "   'tokyo',\n",
       "   'in',\n",
       "   '1968'],\n",
       "  ['he',\n",
       "   'move',\n",
       "   'to',\n",
       "   'new',\n",
       "   'york',\n",
       "   '18',\n",
       "   'years',\n",
       "   'ago',\n",
       "   'study',\n",
       "   'at',\n",
       "   'the',\n",
       "   'tisch',\n",
       "   'school',\n",
       "   'of',\n",
       "   'the',\n",
       "   'arts',\n",
       "   'at',\n",
       "   'new',\n",
       "   'york',\n",
       "   'university',\n",
       "   'and',\n",
       "   'begin',\n",
       "   'his',\n",
       "   'freelance',\n",
       "   'career',\n",
       "   'at',\n",
       "   'the',\n",
       "   'village',\n",
       "   'voice'],\n",
       "  ['he', 'now', 'live', 'in', 'astoria', 'queens'],\n",
       "  ['in',\n",
       "   'camera',\n",
       "   'obscura',\n",
       "   'mr',\n",
       "   'ito',\n",
       "   'describe',\n",
       "   'his',\n",
       "   'practice',\n",
       "   'of',\n",
       "   'store',\n",
       "   'expose',\n",
       "   'film',\n",
       "   'roll',\n",
       "   'in',\n",
       "   'the',\n",
       "   'vegetable',\n",
       "   'compartment',\n",
       "   'of',\n",
       "   'his',\n",
       "   'refrigerator'],\n",
       "  ['every',\n",
       "   'few',\n",
       "   'months',\n",
       "   'i',\n",
       "   'need',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'all',\n",
       "   'these',\n",
       "   'film',\n",
       "   'so',\n",
       "   'that',\n",
       "   'the',\n",
       "   'vegetables',\n",
       "   'can',\n",
       "   'reclaim',\n",
       "   'their',\n",
       "   'space',\n",
       "   'more',\n",
       "   'photos',\n",
       "   'taking',\n",
       "   'pictures',\n",
       "   'by',\n",
       "   'habit',\n",
       "   'hiroyuki',\n",
       "   'ito',\n",
       "   's',\n",
       "   'street',\n",
       "   'photography',\n",
       "   'be',\n",
       "   'feature',\n",
       "   'in',\n",
       "   'a',\n",
       "   'slide',\n",
       "   'show',\n",
       "   'in',\n",
       "   'january',\n",
       "   '2009'],\n",
       "  ['slide',\n",
       "   'show',\n",
       "   'a',\n",
       "   'clueless',\n",
       "   'spectator',\n",
       "   'transfer',\n",
       "   'of',\n",
       "   'guilt',\n",
       "   'be',\n",
       "   'a',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'grids'],\n",
       "  ['each',\n",
       "   'grid',\n",
       "   'contain',\n",
       "   'four',\n",
       "   'snapshots',\n",
       "   'that',\n",
       "   'be',\n",
       "   'take',\n",
       "   'in',\n",
       "   'new',\n",
       "   'york',\n",
       "   'from',\n",
       "   '2006',\n",
       "   'to',\n",
       "   '2009'],\n",
       "  ['they',\n",
       "   'be',\n",
       "   'all',\n",
       "   'shoot',\n",
       "   'in',\n",
       "   'black',\n",
       "   'and',\n",
       "   'white',\n",
       "   'film',\n",
       "   'develop',\n",
       "   'and',\n",
       "   'print',\n",
       "   'on',\n",
       "   'resin',\n",
       "   'coat',\n",
       "   'paper',\n",
       "   'before',\n",
       "   'be',\n",
       "   'scan',\n",
       "   'and',\n",
       "   'assemble',\n",
       "   'into',\n",
       "   'grids'],\n",
       "  ['the',\n",
       "   'idea',\n",
       "   'of',\n",
       "   'make',\n",
       "   'grids',\n",
       "   'come',\n",
       "   'from',\n",
       "   'visit',\n",
       "   'a',\n",
       "   'video',\n",
       "   'booth',\n",
       "   'in',\n",
       "   'times',\n",
       "   'square',\n",
       "   'where',\n",
       "   'the',\n",
       "   'viewer',\n",
       "   'watch',\n",
       "   'four',\n",
       "   'porn',\n",
       "   'movies',\n",
       "   'simultaneously',\n",
       "   'on',\n",
       "   'a',\n",
       "   'split',\n",
       "   'screen'],\n",
       "  ['you',\n",
       "   're',\n",
       "   'suppose',\n",
       "   'to',\n",
       "   'touch',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'screen',\n",
       "   'to',\n",
       "   'indicate',\n",
       "   'which',\n",
       "   'one',\n",
       "   'you',\n",
       "   'want',\n",
       "   'to',\n",
       "   'watch',\n",
       "   'then',\n",
       "   'the',\n",
       "   'split',\n",
       "   'screen',\n",
       "   'turn',\n",
       "   'back',\n",
       "   'to',\n",
       "   'a',\n",
       "   'normal',\n",
       "   'full',\n",
       "   'screen',\n",
       "   'that',\n",
       "   'play',\n",
       "   'the',\n",
       "   'movie',\n",
       "   'of',\n",
       "   'your',\n",
       "   'choice'],\n",
       "  ['i',\n",
       "   'didn',\n",
       "   't',\n",
       "   'choose',\n",
       "   'one',\n",
       "   'over',\n",
       "   'the',\n",
       "   'others',\n",
       "   'because',\n",
       "   'i',\n",
       "   'be',\n",
       "   'afraid',\n",
       "   'of',\n",
       "   'touch',\n",
       "   'the',\n",
       "   'screen',\n",
       "   'which',\n",
       "   'have',\n",
       "   'fingerprint',\n",
       "   'smudge',\n",
       "   'all',\n",
       "   'over',\n",
       "   'from',\n",
       "   'past',\n",
       "   'visitors'],\n",
       "  ['looking',\n",
       "   'at',\n",
       "   'four',\n",
       "   'sad',\n",
       "   'human',\n",
       "   'dramas',\n",
       "   'unfold',\n",
       "   'in',\n",
       "   'front',\n",
       "   'of',\n",
       "   'my',\n",
       "   'eye',\n",
       "   'be',\n",
       "   'at',\n",
       "   'least',\n",
       "   'intellectually',\n",
       "   'stimulate'],\n",
       "  ['upon',\n",
       "   'closer',\n",
       "   'inspection',\n",
       "   'the',\n",
       "   'random',\n",
       "   'movies',\n",
       "   'start',\n",
       "   'to',\n",
       "   'create',\n",
       "   'rhythm',\n",
       "   'of',\n",
       "   'their',\n",
       "   'own',\n",
       "   'both',\n",
       "   'visually',\n",
       "   'and',\n",
       "   'emotionally',\n",
       "   'as',\n",
       "   'if',\n",
       "   'john',\n",
       "   'cage',\n",
       "   'be',\n",
       "   'at',\n",
       "   'work',\n",
       "   'behind',\n",
       "   'the',\n",
       "   'screen'],\n",
       "  ['since',\n",
       "   'my',\n",
       "   'work',\n",
       "   'method',\n",
       "   'in',\n",
       "   'street',\n",
       "   'photography',\n",
       "   'be',\n",
       "   'purely',\n",
       "   'accidental',\n",
       "   'shuffle',\n",
       "   'image',\n",
       "   'into',\n",
       "   'grids',\n",
       "   'underline',\n",
       "   'their',\n",
       "   'arbitrariness'],\n",
       "  ['i',\n",
       "   'only',\n",
       "   'supply',\n",
       "   'clue',\n",
       "   'hop',\n",
       "   'that',\n",
       "   'each',\n",
       "   'viewer',\n",
       "   'come',\n",
       "   'up',\n",
       "   'with',\n",
       "   'his',\n",
       "   'or',\n",
       "   'her',\n",
       "   'own',\n",
       "   'detective',\n",
       "   'novel']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper\n",
    "helper.get_random(\"articles_tom_\",2010,field=\"body_norm\",value={\"$exists\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field \"body_norm\" contains a list of normalized senteces generated from the \"body\" field. Since the body is pre-processed the following steps are:\n",
    "\n",
    "1- Count al the $n$-grams in all the documents from 2010. We are going to use n=5.  \n",
    "2- Keep only the $n$-grams that has a frequency greater than min_tok=100.  \n",
    "3- Save the remaining n-grams in a collectio in mongoDB named \"vocabulary_2010_2010\".\n",
    "\n",
    "The generated count of n-grams is stored in a document which \"\\_id\" is \"2010\". The structure is detailed belown:\n",
    "\n",
    "- \\_id: year\n",
    "- n-grams: number of n-grams to compute\n",
    "- articles: number of articles processed\n",
    "- size: total number of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año 2010\n",
      "Procesando los 1-gramas\n",
      "99 % Estimated time 0 secs.\n",
      "Eliminando tokens que aparecieron menos de 100 veces...\n",
      "Quedaron 17398 tokens.\n",
      "Procesando los 2-gramas\n",
      "99 % Estimated time 0 secs.\n",
      "Eliminando tokens que aparecieron menos de 100 veces...\n",
      "Quedaron 54399 tokens.\n",
      "Procesando los 3-gramas\n",
      "99 % Estimated time 0 secs.\n",
      "Eliminando tokens que aparecieron menos de 100 veces...\n",
      "Quedaron 26893 tokens.\n",
      "Procesando los 4-gramas\n",
      "99 % Estimated time 0 secs.\n",
      "Eliminando tokens que aparecieron menos de 100 veces...\n",
      "Quedaron 6321 tokens.\n",
      "Procesando los 5-gramas\n",
      "99 % Estimated time 0 secs.\n",
      "Eliminando tokens que aparecieron menos de 100 veces...\n",
      "Quedaron 1952 tokens.\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "helper.count_words(\"articles_tom_\",[\"body_norm\"],2010,2010,ngrams=5,min_tok=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "voc=helper.get_vocabulary(\"vocabulary_2010_2010\",2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'n-grams', 'articles', '1-grams', '2-grams', '3-grams', '4-grams', '5-grams', 'size'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to finish computing the vocabulary we are going to filter the n-grams to be kept. The processed vocabulary will be saved in the same collection with \"\\_id\"=\"proc-2010\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quedaron 0 tokens de 5-gramas\n",
      "holis\n",
      "3-grams\n",
      "Quedaron 3 tokens de 4-gramas\n",
      "Quedaron 32 tokens de 3-gramas\n",
      "Quedaron 4373 tokens de 2-gramas\n",
      "Quedaron 17232 tokens de 1-gramas\n",
      "Se salvó el vocabulario compilado en proc-2010\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import helper\n",
    "helper.compile_vocabulary(\"vocabulary_2010_2010\",\"2010\",ngrams=5,min_df=[0.001,0.01,0.05,0.05,0.05],max_df=[10,0.1,0.1,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=helper.get_vocabulary(\"vocabulary_2010_2010\",\"proc-2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493246"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc[\"1-grams\"][\"of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys,values=helper.histo_voc(voc, 10000, plot_from=0, plot_to=100, keys=None,ngrams=3,name=\"histogram\",plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(keys))\n",
    "print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      " year: 2010. 99 % Estimated time 0 secs."
     ]
    }
   ],
   "source": [
    "file=\"data.pck\"\n",
    "prefix=\"articles_tom_\"\n",
    "from_year=2010\n",
    "to_year=2010\n",
    "ngrams=5\n",
    "data,count,dictionary,reverse_dictionary=helper.generate_dataset(file,prefix, from_year,to_year,keys,values,ngrams=ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 742, 252, 14, 3, 6807, 789, 9734, 0, 4555, 2105, 405, 0, 5, 6357, 201, 6198]\n",
      "UNK UNK present us with a mix-of long entries UNK clue and-some challenge UNK in a-wide open grid "
     ]
    }
   ],
   "source": [
    "del voc\n",
    "print(data[2])\n",
    "for word in data[2]:\n",
    "    print(reverse_dictionary[word],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8237"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"curve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "couples=np.zeros([160000000,2],dtype=np.int16)\n",
    "labels=np.zeros(160000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 % Estimated time 0 secs."
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(10001)\n",
    "step=10000\n",
    "start=time.time()\n",
    "cant=len(data)\n",
    "num=0\n",
    "for idx,line in enumerate(data):\n",
    "    c, l = skipgrams(line, 10001, window_size=5, sampling_table=sampling_table,shuffle=False)\n",
    "    if c:\n",
    "        tam=len(l)\n",
    "        couples[num:num+tam,:]=np.array(c,dtype=np.int16)\n",
    "        labels[num:num+tam]=np.array(l,dtype=np.int16)\n",
    "        num=num+tam\n",
    "    if (idx+1)%step==0:\n",
    "        stop = time.time()\n",
    "        estim= (stop-start)*(cant-idx)/idx\n",
    "        sys.stdout.write(\"\\r{} % Estimated time {} secs.\".format(int(idx/cant*100),int(estim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"couples.npy\",couples[0:num,:])\n",
    "np.save(\"labels.npy\",labels[0:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "couples=np.load(\"couples.npy\")\n",
    "labels=np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6515 1937]\n",
      " [6515 1052]\n",
      " [6515   49]\n",
      " [6515  578]\n",
      " [6515    1]\n",
      " [6515 1901]\n",
      " [6515  140]\n",
      " [6515  104]\n",
      " [5082  578]\n",
      " [5082    1]] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cselmo/.conda/envs/doctorado-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/home/cselmo/.conda/envs/doctorado-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration 99000, loss=0.5070355893969533Nearest to trump: club, diamond, it-to, convert, buy, with-one, sell, bath,\n",
      "Nearest to apple: microsoft, computer, apple-s, verizon, blackberry, ipad, google, tablet,\n",
      "Nearest to clinton: secretary-of, the-president, obama, in-washington, treasury, gates, aide, adviser,\n",
      "Nearest to new-york-city: city, manhattan, york, nearby, city-s, district, of-new, historic,\n",
      "Nearest to washington: in-washington, state, john, hill, district, president-of, california, barbara,\n",
      "Nearest to sport: medicine, the-sport, and-in, university, recall, there-be-no, the-olympics, men-s,\n",
      "Nearest to rarely: all-three, few, win, fewer, us, sight, never, often,\n",
      "Nearest to contrast: stark, 3-percent, forecast, digital, rise, sharp, 34, about,\n",
      "Nearest to democrat: representative, a-republican, senator, senate, candidate, the-republican, the-democratic, a-democrat,\n",
      "Nearest to somehow: they, tell-us, and-they, we, away, i-do, so-that, when-i,\n",
      "Nearest to anticipate: be-expect, analysts, ahead, optimistic, investors, unemployment, margins, wall-street,\n",
      "Nearest to the-lead: lead, take-the, win, jonathan, a-lead, 13, lee, ball,\n",
      "Nearest to fuel: emissions, efficient, energy, fossil, gas, natural-gas, electricity, heavy,\n",
      "Nearest to first-round: pick, all-star, packers, rangers, quarterback, game-in, second-round, be-replace,\n",
      "Nearest to probably: weren-t, maybe, end-up, enough, something, he-add, for-it, be-probably,\n",
      "Nearest to for-years: attention, areas, the-system, blame, restore, loom, condition, the-development,\n",
      " Iteration 199000, loss=0.49883459234237654Nearest to trump: diamond, donald, spade, co-op, hearts, desire-to, harlem, empire,\n",
      "Nearest to apple: iphone, jobs, apple-s, the-iphone, google, ipad, amazon, phone,\n",
      "Nearest to clinton: obama, secretary-of, mrs, barack, hillary, presidential, the-president, bush,\n",
      "Nearest to new-york-city: city, district, schools, in-brooklyn, a-city, charter, which-have, teacher,\n",
      "Nearest to washington: florida, texas, kansas, chicago, john, the-president, representative, policy,\n",
      "Nearest to sport: the-sport, shirt, players, men-s, hockey, football, soccer, utility,\n",
      "Nearest to rarely: ignore, have-never, often, interview, complain, besides, they, familiar,\n",
      "Nearest to contrast: stark, classic, reality, with-its, display, reflect, the-ipad, gesture,\n",
      "Nearest to democrat: vote-for, senate, senator, a-democrat, the-republican, the-democratic, representative, republican,\n",
      "Nearest to somehow: sometimes, i-didn, manage-to, grow-up, of-course, one-have, be-all, enough-to,\n",
      "Nearest to anticipate: be-see, the-economy, sooner, agree, interest-rat, although, most-likely, economists,\n",
      "Nearest to the-lead: lead, creative, kevin, producer, team-in, baker, michael, paul,\n",
      "Nearest to fuel: gasoline, water, consumption, energy, electricity, gas, emissions, natural-gas,\n",
      "Nearest to first-round: second-round, all-star, broncos, pick, patriots, round, year-s, game,\n",
      "Nearest to probably: be-better, really, would, so-far, better, worse, not-have, i-know,\n",
      "Nearest to for-years: say-they, be-allow, parent, complain-that, since-then, hungry, politicians, workers,\n",
      " Iteration 299000, loss=0.4940939702093599Nearest to trump: spade, ace, diamond, hearts, casino, resort, club, hotel,\n",
      "Nearest to apple: iphone, ipad, apple-s, software, microsoft, google, developers, tablet,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, bush, barack, ambassador, obama-and, by-mr,\n",
      "Nearest to new-york-city: manhattan, staten-island, city, the-district, city-s, mayor, where-he, department-of,\n",
      "Nearest to washington: in-washington, boston, chicago, florida, the-national, president-obama, rally, officials-in,\n",
      "Nearest to sport: the-sport, soccer, tennis, hockey, football, players, women, vancouver,\n",
      "Nearest to rarely: because-he, do-so, punish, with-their, bother, their-own, anything, but-be,\n",
      "Nearest to contrast: stark, range-of, among, between-the, increase, social, show-the, slight,\n",
      "Nearest to democrat: republican, a-democrat, vote-for, senator, the-republican, congressman, the-democratic, democratic,\n",
      "Nearest to somehow: manage-to, come-back, we-all, myself, i-didn, a-way, he-need, aren-t,\n",
      "Nearest to anticipate: offer, analysts, expect, borrow, an-opportunity, be-expect, the-fed, somewhat,\n",
      "Nearest to the-lead: lead, actor, the-primary, head-of, singer, richard, senior-vice, perform,\n",
      "Nearest to fuel: efficient, increase, gasoline, reduce-the, energy, consumption, fossil, cheap,\n",
      "Nearest to first-round: second-round, pick, lakers, team-to, broncos, the-tournament, matchup, brandon,\n",
      "Nearest to probably: i-would, if-i, i-know, be-go-to, think-that, wouldn-t, think, but-we,\n",
      "Nearest to for-years: decades, have-long, have-never, real-estate, with-all, always-be, the-property, have-begin,\n",
      " Iteration 399000, loss=0.4913836110830307Nearest to trump: club, ace, spade, hate, hearts, diamond, south, how-do,\n",
      "Nearest to apple: iphone, google, the-iphone, device, apple-s, software, owners, ipad,\n",
      "Nearest to clinton: hillary, mrs, netanyahu, barack, paterson, and-mr, bush, by-mr,\n",
      "Nearest to new-york-city: a-city, district, city-s, police, the-mayor, mayor, nyt, manhattan,\n",
      "Nearest to washington: chicago, los-angeles, in-washington, washington-and, boston, state, on-wednesday, report-that,\n",
      "Nearest to sport: hockey, soccer, the-sport, business-and, men-s, players, kid, golf,\n",
      "Nearest to rarely: tend-to, many, and-many, bother, see, often, intense, hardly,\n",
      "Nearest to contrast: stark, gain, 10-percent, company-s, comparison, sentiment, jeff, generally,\n",
      "Nearest to democrat: senator, a-democrat, the-democratic, the-republican, democrat-of, republican-party, republican, west-virginia,\n",
      "Nearest to somehow: must-be, yes, never, very, we-all, realize-that, play-and, to-think,\n",
      "Nearest to anticipate: expect, will-have, larger, analysts, be-expect, bigger, be-much, be-expect-to,\n",
      "Nearest to the-lead: lead, the-primary, jeffrey, kevin, producers, patrick, the-study, producer,\n",
      "Nearest to fuel: gasoline, efficient, increase, energy, dioxide, gas, cars, consumption,\n",
      "Nearest to first-round: the-playoffs, second-round, pick, duke, lakers, draft, devils, in-the-first,\n",
      "Nearest to probably: if-i, i-would, need-a, wouldn-t, but-that, never, that-my, time-i,\n",
      "Nearest to for-years: have-long, decades, hard-to, for-decades, ve-be, and-many, if-they, complain,\n",
      " Iteration 499000, loss=0.4877304653823372Nearest to trump: diamond, ace, donald, club, whether-the, spade, justice, a-particular,\n",
      "Nearest to apple: google, iphone, software, tablet, ipad, the-ipad, intel, jobs,\n",
      "Nearest to clinton: mrs, hillary, and-mr, bush, obama, the-white-house, deputy, from-mr,\n",
      "Nearest to new-york-city: city-s, department-of, manhattan, accord-to-the, city, westchester, for-new, provincial,\n",
      "Nearest to washington: in-washington, officials, lawmakers, president-obama, roy, regulatory, wall-street, efforts-to,\n",
      "Nearest to sport: the-sport, hockey, soccer, basketball, youth, sports, baseball, tennis,\n",
      "Nearest to rarely: never, but-be, too-many, usually, have-never, couldn-t, frequently, be-often,\n",
      "Nearest to contrast: comparison, unique, stark, portray, 52, compare, sharply, index,\n",
      "Nearest to democrat: the-republican, senator, congressman, republican, a-democrat, democratic, dakota, the-democratic,\n",
      "Nearest to somehow: manage-to, don, them-to, to-sit, be-i, fairly, all-those, where-you,\n",
      "Nearest to anticipate: be-expect, smaller, than, analysts, be-much, criticize, approach, expect,\n",
      "Nearest to the-lead: lead, singer, actress, walter, creative, a-lead, have-take, carnegie,\n",
      "Nearest to fuel: gasoline, dioxide, carbon, cars, efficient, vehicles, emissions, consumption,\n",
      "Nearest to first-round: pick, round, second-round, the-2009, the-playoffs, conference, seed, the-tournament,\n",
      "Nearest to probably: time-i, i-ll, t-get, i-would, i-could, anything, anyway, if-that,\n",
      "Nearest to for-years: if-they, because-we, have-work, for-decades, be-around, think-they, communities, rely-on,\n",
      " Iteration 599000, loss=0.48589947932958627Nearest to trump: diamond, king, ace, casino, west, donald, think-about, spade,\n",
      "Nearest to apple: apple-s, intel, google, microsoft, iphone, amazon, google-s, tablet,\n",
      "Nearest to clinton: mrs, hillary, obama-and, obama, obama-s, karzai, barack, presidency,\n",
      "Nearest to new-york-city: community, chatter, today-s, westchester, residents, public-school, accord-to-the, city,\n",
      "Nearest to washington: washington-and, president-obama, brooklyn, wall-street, pittsburgh, in-washington, oct, boston,\n",
      "Nearest to sport: the-sport, culture, basketball, athletes, hockey, football, soccer, race,\n",
      "Nearest to rarely: frequently, eager-to, have-never, be-on-the, time-with, one-another, frequent, never,\n",
      "Nearest to contrast: stark, among, familiar, many-other, obama-s, index, presentation, note-that,\n",
      "Nearest to democrat: senator, a-democrat, representative, democrat-of, dakota, republican, the-republican, a-republican,\n",
      "Nearest to somehow: manage-to, feel, know-how, and-try, far-more, be-they, by-their, of-be,\n",
      "Nearest to anticipate: be-expect, move, the-announcement, even, expect-to, difficulties, rise, larger,\n",
      "Nearest to the-lead: lead, actress, richards, walter, runner, a-lead, alexander, billy,\n",
      "Nearest to fuel: gasoline, gas, efficient, carbon, consumption, electricity, crude, fleet,\n",
      "Nearest to first-round: pick, second-round, finals, n-b, round-of, fifth, game-of, round,\n",
      "Nearest to probably: hurt, i-know, pain, i-ll, because-i, see-that, would-probably, realize-that,\n",
      "Nearest to for-years: and-many, valley, warn-that, forbid, plague, because-we, corruption, harbor,\n",
      " Iteration 699000, loss=0.4840046877264977Nearest to trump: diamond, casino, failures, potential, cash, west, add-to, though,\n",
      "Nearest to apple: apple-s, microsoft, google, versions, company-have, software, kindle, the-ipad,\n",
      "Nearest to clinton: hillary, obama-and, obama, barack, obama-s, bush, deputy, mrs,\n",
      "Nearest to new-york-city: chatter, manhattan, nyt, city, public-school, transit, westchester, the-bronx,\n",
      "Nearest to washington: in-washington, ap, the-editor, officials-in, washington-and, president-obama, heights, 201,\n",
      "Nearest to sport: the-sport, hockey, world-of, athletes, soccer, baseball, football, communities,\n",
      "Nearest to rarely: hold, but-she, confront, often, invitation, never, eager-to, to-face,\n",
      "Nearest to contrast: less, increase, gesture, harsh, comparison, sharply, stark, 4-percent,\n",
      "Nearest to democrat: a-democrat, senator, senate, republican, the-republican, dakota, democratic, chairman,\n",
      "Nearest to somehow: manage-to, survive, all-those, brilliant, self, a-way, by-their, very,\n",
      "Nearest to anticipate: prepare-for, raise, the-announcement, take-advantage, present, this-summer, report-on, higher-than,\n",
      "Nearest to the-lead: lead, the-study, patrick, a-lead, the-primary, the-series, martinez, oregon,\n",
      "Nearest to fuel: gasoline, gas, carbon, consumption, dioxide, emissions, fossil, technologies,\n",
      "Nearest to first-round: second-round, pick, seed, playoff, draft, finals, the-playoffs, advance,\n",
      "Nearest to probably: i-ll, anyway, about-it, i-never, like-that, worse, maybe, than-that,\n",
      "Nearest to for-years: for-decades, last-few, have-just, never-have, have-never, hot, but-have, meet,\n",
      " Iteration 799000, loss=0.47996427482366555Nearest to trump: diamond, spade, tower, condo, ace, hand-and, to-think, an-interest,\n",
      "Nearest to apple: apple-s, google, tablet, microsoft, software, the-ipad, the-iphone, sony,\n",
      "Nearest to clinton: gibbs, obama-s, barack, mrs, biden, hillary, bush, george-w,\n",
      "Nearest to new-york-city: department-of, manhattan, chatter, public-school, borough, departments, city, midtown,\n",
      "Nearest to washington: chicago, lawmakers, officials-in, university, lawyer, elite, white-house, president-obama,\n",
      "Nearest to sport: the-sport, soccer, professional, world-of, football, baseball, technology, athletes,\n",
      "Nearest to rarely: criticize, but-even, recognize, she-have, have-never, or-even, clearly, permit,\n",
      "Nearest to contrast: stark, on-april, suggest, a-loss, forecast, largely, familiar, youtube,\n",
      "Nearest to democrat: a-democrat, senator, dakota, the-republican, representative, republican, west-virginia, senate,\n",
      "Nearest to somehow: all-those, her-own, it-doesn, touch, it-all, one-day, write, you-could,\n",
      "Nearest to anticipate: economists, analysts, expect-to, tablet, arrival, 2011, easier-to, larger,\n",
      "Nearest to the-lead: lead, the-study, roles, previously, a-master, walsh, researcher, singer,\n",
      "Nearest to fuel: fossil, gasoline, dioxide, gas, carbon, oil-and, vehicles, coal,\n",
      "Nearest to first-round: second-round, pick, round-of, playoff, draft, the-playoffs, consecutive, n-b,\n",
      "Nearest to probably: you-could, certainly, possibly, if-it, be-true, or-two, could, the-difference,\n",
      "Nearest to for-years: for-decades, last-few, but-have, be-push, have-work, ve-be, years-and, m-not,\n",
      " Iteration 899000, loss=0.4801405794620513Nearest to trump: club, spade, ace, diamond, place-in, think-about, understand-the, bank-and,\n",
      "Nearest to apple: apple-s, the-iphone, google, iphone, kindle, microsoft, software, mobile,\n",
      "Nearest to clinton: mrs, hillary, gibbs, obama-and, netanyahu, bush, george-w, secretary-of,\n",
      "Nearest to new-york-city: manhattan, public-school, westchester, in-brooklyn, the-district, city-and, chicago, offices,\n",
      "Nearest to washington: los-angeles, officials-in, lawmakers, heights, in-washington, obama-s, university, the-university-of,\n",
      "Nearest to sport: the-sport, professional, hockey, soccer, tournament, world-of, field, baseball,\n",
      "Nearest to rarely: occasionally, speak-to, neither, sometimes, tribal, athletes, actors, usually,\n",
      "Nearest to contrast: stark, be-down, a-loss, sharply, account-of, comparison, this, compare-with,\n",
      "Nearest to democrat: a-democrat, senator, republican, the-republican, democratic, a-republican, west-virginia, indiana,\n",
      "Nearest to somehow: manage-to, her-own, basically, we-all, constant, all-those, be-all, situations,\n",
      "Nearest to anticipate: analysts, expect-to, be-much, of-such, arrival, schedule-to, miss-the, few-months,\n",
      "Nearest to the-lead: lead, a-lead, stanford, producer, the-study, roles, singer, researcher,\n",
      "Nearest to fuel: gas, gasoline, heavy, consumption, natural-gas, dioxide, energy, carbon,\n",
      "Nearest to first-round: pick, round, second-round, break-down, round-of, draft, n-b, steelers,\n",
      "Nearest to probably: win-t, than-that, every-day, have-more, i-d, wouldn-t, could-have, think-we,\n",
      "Nearest to for-years: for-decades, be-around, haven-t, years-and, drink, have-work, years-of, violence,\n",
      " Iteration 999000, loss=0.4810954232215883Nearest to trump: spade, diamond, club, ace, in-part, place-in, anthony, ticket,\n",
      "Nearest to apple: apple-s, microsoft, iphone, mobile, google, developers, google-s, android,\n",
      "Nearest to clinton: hillary, mrs, bush, former-president, obama-and, barack, biden, the-white-house,\n",
      "Nearest to new-york-city: department-of, in-manhattan, the-mayor, police, manhattan, york-s, chatter, public-school,\n",
      "Nearest to washington: officials-in, president-obama, lawmakers, the-national, washington-and, richard, in-washington, ally,\n",
      "Nearest to sport: world-of, the-sport, hockey, amateur, olympics, psychology, tennis, the-olympics,\n",
      "Nearest to rarely: occasionally, routinely, be-often, and-sometimes, time-with, hear-the, wear-a, a-hard,\n",
      "Nearest to contrast: comparison, stark, have-lead, compare-with, compare, decline, pair, sharply,\n",
      "Nearest to democrat: a-democrat, the-republican, senator, the-democratic, republican, representative, democratic, west-virginia,\n",
      "Nearest to somehow: manage-to, enough-to, like-that, in-fact, completely, figure-out, that-to, basically,\n",
      "Nearest to anticipate: analysts, sooner, magnitude, expect-to, initial, prepare-for, despite-the, the-impact,\n",
      "Nearest to the-lead: lead, a-lead, producer, syracuse, starter, john, actor, lender,\n",
      "Nearest to fuel: gas, gasoline, dioxide, emissions, electricity, tax-on, carbon, fossil,\n",
      "Nearest to first-round: second-round, pick, phil, round, playoff, lose-to, upset, championship,\n",
      "Nearest to probably: time-i, actually, bother, except, may-not, wouldn-t, maybe, realize,\n",
      "Nearest to for-years: for-decades, have-work, be-live, long-island, last-few, ve-be, since-then, plague,\n",
      " Iteration 1099000, loss=0.4799395168125636Nearest to trump: spade, diamond, ace, suit, hand-and, executives, and-give, condo,\n",
      "Nearest to apple: apple-s, android, the-iphone, microsoft, mobile, iphone, kindle, software,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, former-president, biden, obama, george-w, white-house,\n",
      "Nearest to new-york-city: in-manhattan, department-of, police, subway, housing, city-s, the-mayor, york-s,\n",
      "Nearest to washington: washington-and, heights, officials-in, williams, white-house, manhattan, cleveland, in-washington,\n",
      "Nearest to sport: the-sport, world-of, hockey, athletic, men-s, football, coach, nascar,\n",
      "Nearest to rarely: often, sometimes, usually, even-in, than-they, cold, difficult-to, frequently,\n",
      "Nearest to contrast: have-lead, comparison, domestic, stark, the-black, compare, sharp, sentiment,\n",
      "Nearest to democrat: senator, the-democratic, republican, a-democrat, a-republican, chairman, the-republican, representative,\n",
      "Nearest to somehow: manage-to, marry, dead, get-it, feel, make-you, a-double, realize-that,\n",
      "Nearest to anticipate: analysts, predict, schedule-to, its-first, stage, it, arrival, in-january,\n",
      "Nearest to the-lead: lead, a-lead, actor, runner, producer, anthony, who-play, citigroup,\n",
      "Nearest to fuel: dioxide, gas, emissions, carbon, fossil, tax-on, pollution, noise,\n",
      "Nearest to first-round: pick, playoff, second-round, round-of, round, broncos, lose-to, team-and,\n",
      "Nearest to probably: actually, still-have, time-i, i-would, anyway, need-a, maybe, would,\n",
      "Nearest to for-years: for-decades, for-several, have-work, m-not, the-region, in-china, to-come, last-few,\n",
      " Iteration 1199000, loss=0.47960571163892757Nearest to trump: spade, club, to-understand, diamond, be-how, poetry, ourselves, desire-to,\n",
      "Nearest to apple: apple-s, the-iphone, android, tablet, iphone, ipad, the-ipad, microsoft,\n",
      "Nearest to clinton: secretary-of, former-president, mrs, george-w, hillary, obama-and, bush, be-mr,\n",
      "Nearest to new-york-city: police, in-brooklyn, city-s, in-manhattan, public-school, housing, transit, subway,\n",
      "Nearest to washington: in-washington, lawmakers, albany, washington-and, white-house, michigan, brad, collins,\n",
      "Nearest to sport: hockey, the-sport, athletic, athletes, men-s, in-other, outfit, york-and,\n",
      "Nearest to rarely: usually, often, engage, occasionally, she, tactics, routinely, t-be,\n",
      "Nearest to contrast: have-lead, stark, domestic, sharp, unlike, be-particularly, sharply, compare-with,\n",
      "Nearest to democrat: senator, a-democrat, the-republican, republican, congressman, democrat-of, senate, a-republican,\n",
      "Nearest to somehow: manage-to, might, get-it, explain, feel, the-message, slowly, sound-like,\n",
      "Nearest to anticipate: quarterly, analysts, economists, expect, rapid, expect-to, prepare-for, bigger,\n",
      "Nearest to the-lead: lead, a-lead, defendants, roles, appoint, take-the, be-take, runner,\n",
      "Nearest to fuel: fossil, gas, gasoline, dioxide, carbon, nuclear, emissions, tax-on,\n",
      "Nearest to first-round: pick, second-round, round, round-of, lose-to, playoff, the-2009, draft,\n",
      "Nearest to probably: either, i-d, need-a, m-not, bother, i-would, still-have, have-more,\n",
      "Nearest to for-years: for-decades, years-and, decades, corruption, for-several, divorce, have-work, have-that,\n",
      " Iteration 1299000, loss=0.47801209428906466Nearest to trump: spade, diamond, approach-to, think-about, family, the-power, overlook, real-estate,\n",
      "Nearest to apple: apple-s, google, jobs, android, tablet, the-iphone, developers, product,\n",
      "Nearest to clinton: hillary, secretary-of, george-w, mrs, aide, former-president, obama-s, gibbs,\n",
      "Nearest to new-york-city: chatter, police, in-brooklyn, in-manhattan, new-jersey, mayor, suburban, september-11,\n",
      "Nearest to washington: washington-and, kirk, officials-in, senators, pennsylvania, manhattan, heights, in-washington,\n",
      "Nearest to sport: the-sport, hockey, the-olympics, in-other, motorcycle, athletes, men-s, games,\n",
      "Nearest to rarely: often, usually, always, be-often, illness, female, tend-to, occasionally,\n",
      "Nearest to contrast: especially-in, sharply, sharp, comparison, charm, even-in, emotional, have-lead,\n",
      "Nearest to democrat: senator, republican, a-democrat, the-republican, senate, the-democratic, congressman, representative,\n",
      "Nearest to somehow: manage-to, survive, marry, character, distinct, alive, how-they, escape,\n",
      "Nearest to anticipate: expect, analysts, economists, predict, at-some, 2010, even, prepare-for,\n",
      "Nearest to the-lead: a-lead, lead, dean, singer, the-offer, deutsche, wells, alexander,\n",
      "Nearest to fuel: emissions, gas, fossil, tensions, carbon, hybrid, dioxide, pollution,\n",
      "Nearest to first-round: pick, second-round, draft, round, round-of, lose-to, the-2009, break-down,\n",
      "Nearest to probably: i-ll, sure, i-would, i-d, i-say, thing, be-go-to, still-have,\n",
      "Nearest to for-years: for-decades, for-several, have-work, be-live, with-him, drink, york-and, to-live,\n",
      " Iteration 1399000, loss=0.47666075393557594Nearest to trump: spade, club, beat, ace, diamond, only-two, the-art, have-long,\n",
      "Nearest to apple: apple-s, microsoft, the-iphone, android, ipad, jobs, youtube, tablet,\n",
      "Nearest to clinton: mrs, hillary, obama-s, secretary-of, former-president, gibbs, biden, obama-and,\n",
      "Nearest to new-york-city: westchester, transit, queens, police-officer, midtown, manhattan, even-in, in-brooklyn,\n",
      "Nearest to washington: washington-and, roy, kirk, university-in, eric, ron, senators, richard,\n",
      "Nearest to sport: the-sport, hockey, sports, soccer, olympic, players, kim, usa,\n",
      "Nearest to rarely: totally, often, usually, occasionally, regularly, always, have-never, seem,\n",
      "Nearest to contrast: domestic, this, seem, especially-in, reflect, comparison, otherwise, blumenthal,\n",
      "Nearest to democrat: a-democrat, the-republican, senator, congressman, republican, democratic, the-democratic, a-republican,\n",
      "Nearest to somehow: manage-to, i-just, feel, there, touch, way, somewhere, do-this,\n",
      "Nearest to anticipate: expect, economists, analysts, most-people, predict, greater, eye, wednesday-that,\n",
      "Nearest to the-lead: lead, a-lead, singer, festival, defendants, syracuse, actress, appoint,\n",
      "Nearest to fuel: fossil, gasoline, carbon, emissions, gas, dioxide, boost, energy-and,\n",
      "Nearest to first-round: second-round, round, round-of, seed, draft, the-tournament, broncos, pick,\n",
      "Nearest to probably: you-ll, see-that, re, i-could, that-i, though, just-about, do,\n",
      "Nearest to for-years: for-decades, for-several, to-live, ve-be, be-around, and-people, spy, in-china,\n",
      " Iteration 1499000, loss=0.4792682215273379Nearest to trump: diamond, spade, double, approach-to, corruption, real-estate, time-and, ace,\n",
      "Nearest to apple: apple-s, android, microsoft, google, the-iphone, google-s, ipad, jobs,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, george-w, aide, obama-s, mr, obama,\n",
      "Nearest to new-york-city: westchester, police-officer, manhattan, city-s, accord-to-the, county, queens, less-than,\n",
      "Nearest to washington: university-in, mutual, washington-and, ryan, group-in, hedge-fund, president-obama, communications,\n",
      "Nearest to sport: the-sport, athletes, sports, hockey, olympic, players, the-olympics, team-and,\n",
      "Nearest to rarely: usually, totally, cheat, always, easier-to, regularly, they, such,\n",
      "Nearest to contrast: domestic, sharply, stark, sharp, evoke, berkeley, decline-in, point-to,\n",
      "Nearest to democrat: republican, senator, congressman, a-democrat, a-republican, representative, democrat-of, the-republican,\n",
      "Nearest to somehow: you-will, the-audience, survive, feel, certainly, manage-to, consistent, i-just,\n",
      "Nearest to anticipate: would, economists, analysts, expect, though-he, most-people, hadn-t, analysis,\n",
      "Nearest to the-lead: lead, a-lead, producers, producer, accord, anthony, the-run, singer,\n",
      "Nearest to fuel: dioxide, boost, fossil, cars, concern-about, nuclear, hybrid, emissions,\n",
      "Nearest to first-round: second-round, round, pick, draft, round-of, team-and, playoff, the-2009,\n",
      "Nearest to probably: we, wasn-t, say-i, ve-see, o-k, m-not, get-out, wouldn-t,\n",
      "Nearest to for-years: for-several, for-decades, with-me, legal, reputation, nurse, terrorism, in-china,\n",
      " Iteration 1599000, loss=0.47711710226535775Nearest to trump: spade, diamond, celebrities, ace, heart, king, worry-about, because-he,\n",
      "Nearest to apple: microsoft, apple-s, the-iphone, developers, jobs, google-s, ipad, google,\n",
      "Nearest to clinton: mrs, secretary-of, the-white-house, hillary, aide, obama-and, former-president, gibbs,\n",
      "Nearest to new-york-city: mayor, while-it, transit, municipal, in-manhattan, westchester, the-east, queens,\n",
      "Nearest to washington: washington-and, rogers, ron, tennessee, portland, murphy, oakland, president-obama,\n",
      "Nearest to sport: the-sport, soccer, hockey, athletes, players, team-and, tennis, men-s,\n",
      "Nearest to rarely: often, resist, easier-to, usually, regularly, routinely, elections, sick,\n",
      "Nearest to contrast: sharply, stark, sharp, domestic, have-lead, unlike, spend-on, point-to,\n",
      "Nearest to democrat: senator, a-democrat, republican, massachusetts, a-republican, representative, the-republican, congressman,\n",
      "Nearest to somehow: flexible, still, the-need, the-community, stimulate, survive, romantic, revolution,\n",
      "Nearest to anticipate: the-future, expect, bigger, next-year, conference, next, so-we, economists,\n",
      "Nearest to the-lead: lead, star, the-offer, be-lead, duke, singer, jim, a-lead,\n",
      "Nearest to fuel: dioxide, gasoline, cars, hybrid, nuclear, fossil, gas, liquid,\n",
      "Nearest to first-round: second-round, pick, round, draft, round-of, the-2009, finals, conference,\n",
      "Nearest to probably: than-that, win-t, m-not, in-my, we, should-have, re, say-you,\n",
      "Nearest to for-years: for-decades, in-china, decades, decade, with-him, for-several, since-then, years-and,\n",
      " Iteration 1699000, loss=0.47665753927826904Nearest to trump: diamond, spade, ace, government-s, heart, real-estate, the-show, have-long,\n",
      "Nearest to apple: apple-s, jobs, microsoft, google, the-iphone, google-s, developers, packard,\n",
      "Nearest to clinton: the-white-house, hillary, secretary-of, mrs, by-mr, president-obama, gibbs, aide,\n",
      "Nearest to new-york-city: the-east, queens, municipal, manhattan, police-officer, new-jersey, city-s, public-school,\n",
      "Nearest to washington: washington-and, president-obama, maryland, mutual, williams, 2010-to, the-editor, bailouts,\n",
      "Nearest to sport: the-sport, tennis, non, athletes, pole, sports, world-of, nascar,\n",
      "Nearest to rarely: have-never, often, time-with, fail-to, by-their, they-didn, easier-to, usually,\n",
      "Nearest to contrast: stark, compare-with, sharply, point-to, size-of, sales-of, unlike, facebook,\n",
      "Nearest to democrat: senator, a-democrat, democratic, congressman, representative, incumbent, republican, democrat-of,\n",
      "Nearest to somehow: plot, communicate, al-qaeda, echo, manage-to, mean, while, we-should,\n",
      "Nearest to anticipate: economists, expect, december, couldn-t, due-to, crisis, the-long, next-year,\n",
      "Nearest to the-lead: lead, producer, singer, a-lead, the-study, the-series, deutsche, actor,\n",
      "Nearest to fuel: gasoline, cars, dioxide, fossil, vehicle, nuclear, emissions, gas,\n",
      "Nearest to first-round: second-round, round, pick, round-of, draft, lose-to, team, the-2009,\n",
      "Nearest to probably: m-not, that-i, wouldn-t, i-would, say-i, think-i, will-probably, not-have,\n",
      "Nearest to for-years: for-decades, abandon, terrorism, pension, grow, ve-be, have-become, supply,\n",
      " Iteration 1799000, loss=0.47826431983709333Nearest to trump: diamond, ace, spade, soho, real-estate, antoni, debt, aesthetic,\n",
      "Nearest to apple: apple-s, microsoft, software, jobs, google-s, google, hardware, the-iphone,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, obama-and, obama-s, the-white-house, aide, white-house,\n",
      "Nearest to new-york-city: city-s, nyt, police, municipal, manhattan, the-east, transit, daily-news,\n",
      "Nearest to washington: washington-and, president-obama, lobbyists, illinois, in-washington, albany, kenneth, lawmakers,\n",
      "Nearest to sport: the-sport, athletes, baseball, also-a, academic, vehicle, football, sports,\n",
      "Nearest to rarely: often, she, have-never, illness, such, they, not-always, athletes,\n",
      "Nearest to contrast: stark, comparison, sharply, emotional, compare-with, especially, fragile, sharp,\n",
      "Nearest to democrat: a-democrat, senator, republican, representative, congressman, democratic, democrat-of, a-republican,\n",
      "Nearest to somehow: echo, mean, crazy, feel, it-all, certainly, we-should, motivate,\n",
      "Nearest to anticipate: expect, economists, analysts, approach, miss-the, explode, acquisitions, crisis,\n",
      "Nearest to the-lead: lead, the-study, producer, john, kyle, producers, actress, walter,\n",
      "Nearest to fuel: gasoline, fossil, emissions, tensions, rise-in, gas, dioxide, coal,\n",
      "Nearest to first-round: pick, second-round, round-of, round, draft, the-2009, rookie, year-s,\n",
      "Nearest to probably: i-would, in-my, certainly, should-have, say-i, i, think-i, we,\n",
      "Nearest to for-years: for-decades, ve-be, try, reform, decade, complain, long, to-live,\n",
      " Iteration 1899000, loss=0.4755265471935266Nearest to trump: diamond, spade, ace, aesthetic, club, real-estate, bid, trick,\n",
      "Nearest to apple: apple-s, microsoft, jobs, google, google-s, the-iphone, software, developers,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, obama-s, aide, obama, as-mr, obama-and,\n",
      "Nearest to new-york-city: city-s, manhattan, nyt, county, city, the-bronx, mayor, the-show,\n",
      "Nearest to washington: washington-and, illinois, lawmakers, the-political, in-washington, president-obama, kenneth, maryland,\n",
      "Nearest to sport: the-sport, hockey, men-s, sports, players, baseball, fan, and-where,\n",
      "Nearest to rarely: often, easier-to, athletes, complain-that, sometimes, typically, they, they-didn,\n",
      "Nearest to contrast: emotional, the-experience, stark, who-run, rise, much, especially, size-of,\n",
      "Nearest to democrat: a-democrat, senator, republican, representative, congressman, democratic, a-republican, democrat-of,\n",
      "Nearest to somehow: unless, deeply, manage-to, extend, universe, stimulate, hair, practically,\n",
      "Nearest to anticipate: analysts, expect, economists, miss-the, bigger, be-much, next-year, it,\n",
      "Nearest to the-lead: lead, producer, kyle, singer, alexander, actress, portray, the-start,\n",
      "Nearest to fuel: gas, emissions, dioxide, fossil, gasoline, boost, pump, car,\n",
      "Nearest to first-round: pick, second-round, round-of, round, lose-to, bears, the-2009, past-the,\n",
      "Nearest to probably: should-have, i-would, that-i, we, be-here, i, think-i, i-ll,\n",
      "Nearest to for-years: for-decades, have-long, abandon, pipeline, who-work, which-have, valley, crack,\n",
      " Iteration 1999000, loss=0.47361266297101995Nearest to trump: diamond, spade, heart, always, ace, real-estate, trick, aesthetic,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, developers, jobs, software, ipad,\n",
      "Nearest to clinton: hillary, secretary-of, bush, mrs, biden, meet-with, obama-s, aide,\n",
      "Nearest to new-york-city: city-s, city, transit, city-and, county, mayor, borough, 10-years,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, mutual, chicago, on-tuesday, group-in, trade,\n",
      "Nearest to sport: the-sport, olympics, athletic, players, hockey, women-s, athletes, horse,\n",
      "Nearest to rarely: typically, athletes, sometimes, be-often, often, productions, easy-to, she,\n",
      "Nearest to contrast: stark, emotional, attitude, modest, comparison, reflect, which-mr, achieve,\n",
      "Nearest to democrat: senator, republican, democratic, representative, a-democrat, a-republican, the-republican, senate,\n",
      "Nearest to somehow: manage-to, scrap, i-just, it-work, and-yet, deeply, it-all, that-seem,\n",
      "Nearest to anticipate: expect, analysts, economists, further, initial, next-year, production, creation,\n",
      "Nearest to the-lead: lead, producer, citigroup, a-lead, producers, award, to-begin, second,\n",
      "Nearest to fuel: fossil, gas, emissions, efficient, efficiency, despite, demand-for, nuclear,\n",
      "Nearest to first-round: pick, second-round, round-of, round, finish, lose-to, the-playoffs, broncos,\n",
      "Nearest to probably: wouldn-t, think-i, i, we, i-would, sure, certainly, have-never,\n",
      "Nearest to for-years: for-decades, to-remain, already, fight, main, have-become, have-work, haven-t,\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam,rmsprop\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = logs\n",
    "    summary_value.tag = names\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()\n",
    "\n",
    "log_path = './logs'\n",
    "callback = TensorBoard(log_path)\n",
    "\n",
    "vocab_size = 10001\n",
    "vector_dim = 300\n",
    "epochs = 2000000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 5000  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "word_target = couples[:,0]\n",
    "word_context = couples[:,1]\n",
    "labels=labels[:num]\n",
    "valid_examples[0]=6761 #trump\n",
    "valid_examples[1]=1269 # apple\n",
    "valid_examples[2]=1991 #clinton\n",
    "valid_examples[3]=834 #new-york-city\n",
    "valid_examples[4]=597 #washington\n",
    "print(couples[:10], labels[:10])\n",
    "\n",
    "# create some input variables\n",
    "input_target = Input(shape=(1,))\n",
    "input_context = Input(shape=(1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "similarity = dot([target, context], normalize=True, axes=0)\n",
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)\n",
    "\n",
    "callback.set_model(model)\n",
    "train_names = ['train_loss', 'train_mae']\n",
    "val_names = ['val_loss', 'val_mae']\n",
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model)\n",
    "train_names = 'train_loss'\n",
    "batch_size=256\n",
    "arr_1 = np.zeros((batch_size,))\n",
    "arr_2 = np.zeros((batch_size,))\n",
    "arr_3 = np.zeros((batch_size,))\n",
    "aux_loss=0\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1,batch_size)\n",
    "    arr_1[:,] = word_target[idx]\n",
    "    arr_2[:,] = word_context[idx]\n",
    "    arr_3[:,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    aux_loss=aux_loss+loss/1000\n",
    "    write_log(callback, train_names, loss, cnt)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"\\r Iteration {}, loss={}\".format(cnt, aux_loss),end=\"\")\n",
    "        aux_loss=0\n",
    "    if (cnt+1) % 100000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest to trump: diamond, spade, heart, always, ace, real-estate, aesthetic, trick,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, jobs, developers, software, ipad,\n",
      "Nearest to clinton: hillary, secretary-of, bush, mrs, biden, meet-with, obama-s, aide,\n",
      "Nearest to new-york-city: city-s, city, transit, county, city-and, mayor, a-city, borough,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, chicago, mutual, on-tuesday, group-in, albany,\n",
      "Nearest to sport: the-sport, players, olympics, athletic, hockey, athletes, horse, women-s,\n",
      "Nearest to rarely: typically, athletes, sometimes, be-often, often, productions, easy-to, she,\n",
      "Nearest to contrast: stark, emotional, attitude, modest, comparison, reflect, achieve, which-mr,\n",
      "Nearest to democrat: senator, republican, democratic, representative, a-democrat, a-republican, the-republican, senate,\n",
      "Nearest to somehow: manage-to, scrap, i-just, it-work, and-yet, it-but, that-seem, deeply,\n",
      "Nearest to anticipate: expect, analysts, economists, further, initial, production, next-year, we-would,\n",
      "Nearest to the-lead: lead, producer, citigroup, a-lead, producers, award, to-begin, second,\n",
      "Nearest to fuel: fossil, gas, emissions, efficient, efficiency, despite, demand-for, nuclear,\n",
      "Nearest to first-round: pick, second-round, round-of, round, lose-to, finish, the-playoffs, broncos,\n",
      "Nearest to probably: wouldn-t, think-i, we, i, i-would, sure, have-never, anyway,\n",
      "Nearest to for-years: for-decades, to-remain, already, fight, have-become, main, have-work, plague,\n",
      "Iteration 2099000, loss=0.47240908348560284\n",
      "Nearest to trump: diamond, heart, always, spade, ace, trick, maybe, have-have,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, developers, jobs, software, apps,\n",
      "Nearest to clinton: hillary, secretary-of, bush, mrs, biden, obama-s, obama, as-mr,\n",
      "Nearest to new-york-city: city-s, city, transit, mayor, borough, city-and, county, campus,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, chicago, on-tuesday, mutual, york, group-in,\n",
      "Nearest to sport: the-sport, horse, hockey, olympics, men-s, athletic, athletes, players,\n",
      "Nearest to rarely: typically, athletes, be-often, often, runners, easier-to, productions, sometimes,\n",
      "Nearest to contrast: stark, reflect, achieve, emotional, attitude, picture, point-to, modest,\n",
      "Nearest to democrat: senator, democratic, republican, a-democrat, representative, a-republican, the-republican, democrat-of,\n",
      "Nearest to somehow: i-just, manage-to, scrap, arrangements, get-it, ways, it-but, it-all,\n",
      "Nearest to anticipate: expect, analysts, further, economists, initial, stage, production, next-year,\n",
      "Nearest to the-lead: lead, producer, citigroup, producers, a-lead, award, cause-of, second,\n",
      "Nearest to fuel: gas, fossil, emissions, nuclear, efficient, manufacture, curb, demand-for,\n",
      "Nearest to first-round: pick, second-round, round-of, round, finish, broncos, lose-to, year-s,\n",
      "Nearest to probably: wouldn-t, think-i, we, i-would, i, never-have, so-i, certainly,\n",
      "Nearest to for-years: for-decades, already, to-remain, fight, plague, main, have-become, which-have,\n",
      "Iteration 2199000, loss=0.471712599605322\n",
      "Nearest to trump: diamond, heart, always, aesthetic, spade, sovereign, trick, ace,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, jobs, developers, software, android,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, bush, biden, obama-s, bill-clinton, as-mr,\n",
      "Nearest to new-york-city: city-s, city, transit, chatter, mayor, campus, borough, or-less,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, mutual, on-tuesday, york, chicago, senators,\n",
      "Nearest to sport: the-sport, horse, hockey, men-s, baseball, athletic, olympics, athletes,\n",
      "Nearest to rarely: athletes, typically, often, be-often, easy-to, be-usually, she, time-with,\n",
      "Nearest to contrast: stark, reflect, emotional, attitude, achieve, point-to, modest, picture,\n",
      "Nearest to democrat: senator, republican, democratic, a-democrat, a-republican, representative, the-republican, senate,\n",
      "Nearest to somehow: manage-to, i-just, scrap, arrangements, capable-of, get-it, differently, it-but,\n",
      "Nearest to anticipate: expect, analysts, economists, initial, december, further, stage, be-much,\n",
      "Nearest to the-lead: lead, citigroup, producer, producers, a-lead, cause-of, portray, award,\n",
      "Nearest to fuel: gas, fossil, emissions, nuclear, efficient, gasoline, dioxide, efficiency,\n",
      "Nearest to first-round: pick, second-round, round-of, lose-to, round, broncos, finish, year-s,\n",
      "Nearest to probably: wouldn-t, think-i, never-have, he-d, i-would, we, time-i, than-that,\n",
      "Nearest to for-years: for-decades, to-remain, fight, plague, main, already, work-for, lately,\n",
      "Iteration 2299000, loss=0.4734070001542565\n",
      "Nearest to trump: diamond, spade, heart, aesthetic, real-estate, ace, always, evoke,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, developers, iphone, apps,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, bush, biden, obama-s, obama, the-white-house,\n",
      "Nearest to new-york-city: city-s, city, transit, mayor, campus, county, residents, chatter,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, mutual, on-tuesday, university-in,\n",
      "Nearest to sport: the-sport, horse, hockey, olympics, players, men-s, baseball, athletic,\n",
      "Nearest to rarely: typically, athletes, sometimes, be-usually, often, be-often, easy-to, time-with,\n",
      "Nearest to contrast: stark, achieve, attitude, emotional, reflect, modest, comparison, which-mr,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, representative, a-republican, the-republican, senate,\n",
      "Nearest to somehow: i-just, scrap, manage-to, deeply, arrangements, it-but, capable-of, get-it,\n",
      "Nearest to anticipate: expect, economists, next-year, further, analysts, stage, extent, acquisitions,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producer, producers, a-lead, deutsche, lender,\n",
      "Nearest to fuel: gas, emissions, nuclear, fossil, efficient, cars, gasoline, dioxide,\n",
      "Nearest to first-round: pick, second-round, round-of, lose-to, round, broncos, team, be-replace,\n",
      "Nearest to probably: wouldn-t, think-i, we, he-d, i-would, never-have, anyway, than-that,\n",
      "Nearest to for-years: for-decades, fight, to-remain, ve-be, plague, have-work, already, lately,\n",
      "Iteration 2399000, loss=0.4730165263712403\n",
      "Nearest to trump: diamond, spade, sovereign, real-estate, ace, aesthetic, heart, still-the,\n",
      "Nearest to apple: apple-s, microsoft, google, the-iphone, jobs, the-ipad, developers, iphone,\n",
      "Nearest to clinton: hillary, secretary-of, bush, mrs, biden, obama-s, obama, bill-clinton,\n",
      "Nearest to new-york-city: city-s, transit, city, county, represent, westchester, borough, mayor,\n",
      "Nearest to washington: washington-and, president-obama, chicago, york, in-washington, mutual, senators, timothy,\n",
      "Nearest to sport: the-sport, horse, hockey, men-s, players, sports, baseball, athletes,\n",
      "Nearest to rarely: typically, athletes, often, sometimes, easier-to, be-usually, be-often, such,\n",
      "Nearest to contrast: stark, achieve, modest, size-of, emotional, attitude, reflect, comparison,\n",
      "Nearest to democrat: senator, republican, democratic, a-republican, a-democrat, representative, the-republican, incumbent,\n",
      "Nearest to somehow: i-just, scrap, manage-to, arrangements, many-people, a-piece, it-but, capable-of,\n",
      "Nearest to anticipate: expect, economists, next-year, analysts, december, stage, loom, further,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producer, producers, deutsche, award, portray,\n",
      "Nearest to fuel: gas, emissions, nuclear, fossil, efficient, demand-for, manufacture, cars,\n",
      "Nearest to first-round: second-round, pick, round-of, be-replace, round, lose-to, champion, matchup,\n",
      "Nearest to probably: wouldn-t, think-i, anyway, than-that, he-d, we, i-would, never-have,\n",
      "Nearest to for-years: for-decades, plague, ve-be, fight, 30-years, to-remain, have-become, lately,\n",
      "Iteration 2499000, loss=0.47144451254606273\n",
      "Nearest to trump: diamond, spade, real-estate, ace, sovereign, aesthetic, heart, always,\n",
      "Nearest to apple: apple-s, microsoft, google, the-iphone, jobs, the-ipad, iphone, developers,\n",
      "Nearest to clinton: hillary, secretary-of, obama, biden, bush, mrs, obama-s, meet-with,\n",
      "Nearest to new-york-city: city-s, county, transit, city, borough, campus, mayor, city-and,\n",
      "Nearest to washington: washington-and, president-obama, chicago, albany, york, in-washington, illinois, ron,\n",
      "Nearest to sport: the-sport, men-s, horse, players, olympics, baseball, hockey, athletes,\n",
      "Nearest to rarely: typically, athletes, sometimes, often, be-often, easier-to, such, pay-to,\n",
      "Nearest to contrast: stark, size-of, comparison, chic, which-mr, sharply, emotional, achieve,\n",
      "Nearest to democrat: senator, republican, the-republican, a-democrat, democratic, representative, a-republican, incumbent,\n",
      "Nearest to somehow: i-just, scrap, manage-to, arrangements, deeply, unless, it-but, capable-of,\n",
      "Nearest to anticipate: expect, economists, next-year, analysts, loom, stage, bigger, acquisitions,\n",
      "Nearest to the-lead: lead, citigroup, producer, award, cause-of, producers, albert, portray,\n",
      "Nearest to fuel: gas, emissions, efficient, nuclear, manufacture, dioxide, despite, fossil,\n",
      "Nearest to first-round: pick, second-round, round-of, matchup, the-playoffs, be-replace, lose-to, bears,\n",
      "Nearest to probably: wouldn-t, anyway, than-that, never-have, think-i, he-d, i-know, we,\n",
      "Nearest to for-years: already, to-remain, fight, have-become, for-decades, main, pension, which-have,\n",
      "Iteration 2599000, loss=0.47121930521726585\n",
      "Nearest to trump: diamond, ace, spade, heart, aesthetic, real-estate, sovereign, evoke,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, iphone, developers,\n",
      "Nearest to clinton: hillary, secretary-of, bush, obama, biden, mrs, obama-s, aide,\n",
      "Nearest to new-york-city: city-s, city, transit, county, mayor, residents, manhattan, police,\n",
      "Nearest to washington: washington-and, president-obama, chicago, albany, york, in-washington, illinois, kenneth,\n",
      "Nearest to sport: the-sport, men-s, athletes, horse, players, hockey, olympics, vehicle,\n",
      "Nearest to rarely: typically, athletes, sometimes, often, be-often, pay-to, time-with, easier-to,\n",
      "Nearest to contrast: stark, comparison, which-mr, emotional, achieve, attitude, size-of, sharply,\n",
      "Nearest to democrat: senator, republican, a-democrat, the-republican, a-republican, democratic, representative, incumbent,\n",
      "Nearest to somehow: i-just, scrap, manage-to, arrangements, far-more, it-but, feel, many-people,\n",
      "Nearest to anticipate: expect, economists, next-year, analysts, loom, it, stage, bigger,\n",
      "Nearest to the-lead: lead, citigroup, albert, producers, cause-of, producer, award, previously,\n",
      "Nearest to fuel: gas, efficient, demand-for, emissions, manufacture, nuclear, despite, fossil,\n",
      "Nearest to first-round: second-round, pick, the-playoffs, round-of, be-replace, matchup, lose-to, round,\n",
      "Nearest to probably: wouldn-t, we, than-that, never-have, anyway, he-d, be-there, think-i,\n",
      "Nearest to for-years: for-decades, have-become, already, to-remain, fight, lately, haven-t, main,\n",
      "Iteration 2699000, loss=0.47137734103202855\n",
      "Nearest to trump: diamond, spade, ace, sovereign, heart, real-estate, aesthetic, suit,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, developers, android,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, biden, obama-s, bush, as-mr, the-white-house,\n",
      "Nearest to new-york-city: city-s, city, transit, county, mayor, a-city, manhattan, campus,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, albany, the-washington, public-relations,\n",
      "Nearest to sport: the-sport, men-s, athletes, horse, hockey, baseball, olympics, sports,\n",
      "Nearest to rarely: often, typically, athletes, sometimes, be-often, easier-to, actors, time-with,\n",
      "Nearest to contrast: comparison, stark, achieve, which-mr, size-of, sharply, attitude, emotional,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, representative, the-republican, a-republican, senate,\n",
      "Nearest to somehow: scrap, i-just, manage-to, arrangements, deeply, it-but, a-piece, far-more,\n",
      "Nearest to anticipate: expect, economists, next-year, loom, analysts, stage, further, bigger,\n",
      "Nearest to the-lead: lead, citigroup, award, cause-of, previously, albert, producer, producers,\n",
      "Nearest to fuel: gas, emissions, efficient, demand-for, manufacture, gasoline, boost, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, be-replace, the-playoffs, conference, matchup,\n",
      "Nearest to probably: wouldn-t, we, anyway, think-i, i, never-have, sure, so-i,\n",
      "Nearest to for-years: have-become, for-decades, already, fight, ve-be, to-remain, main, long,\n",
      "Iteration 2799000, loss=0.4708714021444324\n",
      "Nearest to trump: diamond, spade, real-estate, sovereign, ace, aesthetic, suit, evoke,\n",
      "Nearest to apple: apple-s, google, microsoft, jobs, the-iphone, iphone, the-ipad, developers,\n",
      "Nearest to clinton: hillary, secretary-of, mrs, biden, obama-s, obama, bush, aide,\n",
      "Nearest to new-york-city: city-s, city, transit, mayor, county, manhattan, city-and, residents,\n",
      "Nearest to washington: washington-and, president-obama, york, albany, chicago, in-washington, kenneth, senators,\n",
      "Nearest to sport: the-sport, horse, olympics, baseball, men-s, sports, hockey, hat,\n",
      "Nearest to rarely: typically, athletes, often, sometimes, be-usually, be-often, productions, easier-to,\n",
      "Nearest to contrast: stark, comparison, sharply, achieve, size-of, which-mr, attitude, chic,\n",
      "Nearest to democrat: senator, republican, a-democrat, representative, democratic, a-republican, the-republican, incumbent,\n",
      "Nearest to somehow: i-just, scrap, arrangements, manage-to, humans, far-more, capable-of, differently,\n",
      "Nearest to anticipate: expect, economists, next-year, further, stage, analysts, loom, bigger,\n",
      "Nearest to the-lead: lead, citigroup, producer, producers, albert, cause-of, lopez, previously,\n",
      "Nearest to fuel: gas, demand-for, emissions, fossil, gasoline, manufacture, boost, efficient,\n",
      "Nearest to first-round: second-round, pick, lose-to, round-of, round, conference, be-replace, year-s,\n",
      "Nearest to probably: wouldn-t, we, anyway, think-i, i, than-that, so-i, i-know,\n",
      "Nearest to for-years: have-become, for-decades, to-remain, fight, ve-be, already, lately, 30-years,\n",
      "Iteration 2899000, loss=0.4709318260252481\n",
      "Nearest to trump: diamond, real-estate, sovereign, suit, aesthetic, evoke, still-the, spade,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, jobs, iphone, the-ipad, android,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama-s, obama, biden, bush, gibbs,\n",
      "Nearest to new-york-city: city-s, city, transit, county, residents, mayor, manhattan, a-city,\n",
      "Nearest to washington: washington-and, president-obama, york, albany, kirk, senators, in-washington, center-on,\n",
      "Nearest to sport: the-sport, men-s, baseball, players, sports, athletes, hockey, horse,\n",
      "Nearest to rarely: typically, athletes, often, sometimes, have-never, easier-to, occasionally, be-usually,\n",
      "Nearest to contrast: stark, attitude, comparison, size-of, achieve, modest, which-mr, sharply,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, the-republican, representative, a-republican, senate,\n",
      "Nearest to somehow: arrangements, scrap, i-just, it-but, manage-to, of-course, forgive, a-piece,\n",
      "Nearest to anticipate: expect, economists, next-year, stage, further, extent, analysts, bigger,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producer, albert, deutsche, producers, division,\n",
      "Nearest to fuel: gas, emissions, gasoline, manufacture, nuclear, fossil, efficient, cars,\n",
      "Nearest to first-round: second-round, pick, round-of, round, conference, year-s, matchup, be-replace,\n",
      "Nearest to probably: we, wouldn-t, than-that, never-have, think-i, i, anyway, so-i,\n",
      "Nearest to for-years: have-become, for-decades, lately, to-remain, the-obama, haven-t, already, main,\n",
      "Iteration 2999000, loss=0.47133530810475344\n",
      "Nearest to trump: diamond, real-estate, sovereign, suit, evoke, spade, ace, maybe,\n",
      "Nearest to apple: apple-s, google, microsoft, jobs, the-iphone, iphone, developers, the-ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama-s, obama, gibbs, as-mr, biden,\n",
      "Nearest to new-york-city: city-s, city, county, mayor, transit, manhattan, residents, city-and,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, kirk, albany, timothy, in-washington,\n",
      "Nearest to sport: the-sport, men-s, hockey, players, athletes, baseball, olympics, sports,\n",
      "Nearest to rarely: typically, athletes, often, easier-to, sometimes, be-often, such, be-usually,\n",
      "Nearest to contrast: stark, comparison, attitude, chic, which-mr, size-of, reflect, sharply,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, the-republican, representative, a-republican, senate,\n",
      "Nearest to somehow: scrap, arrangements, i-just, it-but, manage-to, you-will, it-all, a-piece,\n",
      "Nearest to anticipate: expect, economists, analysts, stage, next-year, it, further, know-if,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, albert, producer, producers, lopez, portray,\n",
      "Nearest to fuel: gas, emissions, gasoline, fossil, manufacture, efficient, nuclear, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, team, champion, conference, the-playoffs,\n",
      "Nearest to probably: wouldn-t, we, i, think-i, than-that, certainly, sure, that-may,\n",
      "Nearest to for-years: to-remain, for-decades, have-become, ve-be, fight, haven-t, the-things, the-obama,\n",
      "Iteration 3099000, loss=0.4707668623328201\n",
      "Nearest to trump: diamond, real-estate, sovereign, spade, rise-to, heart, suit, evoke,\n",
      "Nearest to apple: apple-s, google, microsoft, jobs, the-iphone, iphone, the-ipad, developers,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, bush, gibbs, aide, obama-s, biden,\n",
      "Nearest to new-york-city: city-s, city, transit, mayor, county, city-and, police, nyt,\n",
      "Nearest to washington: washington-and, president-obama, kirk, in-washington, chicago, albany, mutual, ron,\n",
      "Nearest to sport: the-sport, athletes, baseball, sports, men-s, hockey, players, cars,\n",
      "Nearest to rarely: typically, athletes, often, few, easier-to, such, sometimes, be-often,\n",
      "Nearest to contrast: stark, modest, comparison, attitude, chic, size-of, which-mr, emotional,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, representative, a-republican, the-republican, senate,\n",
      "Nearest to somehow: scrap, arrangements, you-will, ways, manage-to, i-just, it-but, and-yet,\n",
      "Nearest to anticipate: expect, economists, analysts, it, stage, extent, next-year, further,\n",
      "Nearest to the-lead: lead, citigroup, albert, producers, producer, cause-of, award, lopez,\n",
      "Nearest to fuel: gas, emissions, demand-for, fossil, manufacture, gasoline, efficient, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, round, conference, team, the-playoffs, lose-to,\n",
      "Nearest to probably: wouldn-t, we, think-i, i, i-would, than-that, we-don, sure,\n",
      "Nearest to for-years: to-remain, for-decades, haven-t, for-several, ve-be, have-become, the-things, lately,\n",
      "Iteration 3199000, loss=0.47144691884517637\n",
      "Nearest to trump: diamond, real-estate, spade, rise-to, always, trick, sovereign, approach-to,\n",
      "Nearest to apple: apple-s, google, microsoft, jobs, the-iphone, the-ipad, iphone, apps,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, biden, bush, gibbs, obama-s, obama,\n",
      "Nearest to new-york-city: city-s, transit, county, city, mayor, residents, borough, westchester,\n",
      "Nearest to washington: washington-and, president-obama, kirk, albany, chicago, ron, senators, york,\n",
      "Nearest to sport: the-sport, athletes, horse, hockey, sports, men-s, vehicle, hall-of,\n",
      "Nearest to rarely: typically, often, athletes, have-never, sometimes, few, easier-to, be-usually,\n",
      "Nearest to contrast: comparison, stark, size-of, attitude, which-mr, emotional, this, chic,\n",
      "Nearest to democrat: senator, a-democrat, democratic, republican, representative, the-republican, a-republican, senate,\n",
      "Nearest to somehow: scrap, manage-to, arrangements, ways, i-just, it-but, extend, forgive,\n",
      "Nearest to anticipate: expect, analysts, economists, it, stage, loom, further, next-year,\n",
      "Nearest to the-lead: lead, citigroup, producer, albert, cause-of, producers, cruz, wells,\n",
      "Nearest to fuel: gas, emissions, fossil, manufacture, nuclear, gasoline, efficient, demand-for,\n",
      "Nearest to first-round: second-round, pick, round-of, round, team, conference, draft, lose-to,\n",
      "Nearest to probably: we, wouldn-t, certainly, i-would, than-that, think-i, anyway, i,\n",
      "Nearest to for-years: for-decades, to-remain, the-obama, have-become, ve-be, fight, lately, the-things,\n",
      "Iteration 3299000, loss=0.4706655232310296\n",
      "Nearest to trump: diamond, always, real-estate, sovereign, trick, spade, approach-to, suit,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, iphone, developers,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, biden, bush, obama, gibbs, obama-and,\n",
      "Nearest to new-york-city: city-s, county, transit, city, mayor, nyt, residents, city-and,\n",
      "Nearest to washington: washington-and, president-obama, chicago, in-washington, albany, york, state-have, kirk,\n",
      "Nearest to sport: the-sport, hockey, athletes, sports, players, horse, baseball, men-s,\n",
      "Nearest to rarely: typically, often, athletes, easier-to, sometimes, have-never, such, be-usually,\n",
      "Nearest to contrast: attitude, stark, achieve, modest, comparison, reflect, emotional, general,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, representative, congressman, senate, the-republican,\n",
      "Nearest to somehow: scrap, arrangements, i-just, of-them, it-but, deeply, manage-to, forgive,\n",
      "Nearest to anticipate: expect, analysts, economists, next-year, loom, it, further, stage,\n",
      "Nearest to the-lead: lead, citigroup, producers, albert, cause-of, previously, producer, wells,\n",
      "Nearest to fuel: gas, emissions, efficient, fossil, manufacture, gasoline, nuclear, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, team, conference, draft, champion,\n",
      "Nearest to probably: wouldn-t, we, i-would, think-i, say-you, i, certainly, so-i,\n",
      "Nearest to for-years: for-decades, lately, fight, ve-be, complain, have-become, to-remain, the-obama,\n",
      "Iteration 3399000, loss=0.4705011380314824\n",
      "Nearest to trump: diamond, always, real-estate, suit, spade, donald, 64, sovereign,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, iphone, apps,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, gibbs, bush, aide, obama-and, as-mr,\n",
      "Nearest to new-york-city: transit, county, city, city-s, nyt, mayor, police, city-and,\n",
      "Nearest to washington: washington-and, president-obama, york, albany, chicago, kirk, in-washington, senators,\n",
      "Nearest to sport: the-sport, hockey, athletes, men-s, players, baseball, general-manager, sports,\n",
      "Nearest to rarely: often, typically, athletes, sometimes, easier-to, few, have-never, such,\n",
      "Nearest to contrast: stark, modest, attitude, reflect, comparison, achieve, stand-in, emotional,\n",
      "Nearest to democrat: senator, democratic, a-democrat, republican, representative, congressman, senate, a-republican,\n",
      "Nearest to somehow: scrap, i-just, arrangements, you-will, manage-to, ways, deeply, extend,\n",
      "Nearest to anticipate: expect, economists, stage, analysts, next-year, it, acquisitions, know-if,\n",
      "Nearest to the-lead: lead, citigroup, producer, producers, cause-of, singer, kyle, wells,\n",
      "Nearest to fuel: gas, fossil, emissions, efficient, gasoline, manufacture, nuclear, demand-for,\n",
      "Nearest to first-round: second-round, pick, round-of, round, team, conference, lose-to, champion,\n",
      "Nearest to probably: we, wouldn-t, i-would, think-i, say-you, i, certainly, so-i,\n",
      "Nearest to for-years: for-decades, ve-be, long, fight, lately, have-become, for-several, to-remain,\n",
      "Iteration 3499000, loss=0.4686609789729121\n",
      "Nearest to trump: diamond, always, spade, suit, sovereign, be-important, trick, donald,\n",
      "Nearest to apple: apple-s, google, microsoft, the-iphone, jobs, the-ipad, iphone, developers,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, biden, obama-and, bush, obama-s,\n",
      "Nearest to new-york-city: city-s, county, transit, mayor, city, chatter, police, nyt,\n",
      "Nearest to washington: washington-and, president-obama, york, albany, in-washington, senators, chicago, ron,\n",
      "Nearest to sport: the-sport, hockey, athletes, baseball, sports, horse, players, general-manager,\n",
      "Nearest to rarely: often, sometimes, typically, athletes, easier-to, have-never, be-usually, such,\n",
      "Nearest to contrast: stark, comparison, attitude, achieve, emotional, size-of, chic, charm,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, congressman, senate, representative, support-of,\n",
      "Nearest to somehow: scrap, arrangements, deeply, i-just, you-will, it-all, dead, ways,\n",
      "Nearest to anticipate: expect, economists, analysts, next-year, stage, it, loom, the-begin,\n",
      "Nearest to the-lead: lead, citigroup, producer, cause-of, producers, albert, singer, portray,\n",
      "Nearest to fuel: gas, emissions, fossil, gasoline, efficient, carbon, nuclear, demand-for,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, team, champion, matchup,\n",
      "Nearest to probably: we, wouldn-t, think-i, i-would, i, anyway, so-i, say-i,\n",
      "Nearest to for-years: ve-be, for-decades, long, fight, lately, to-remain, for-several, have-become,\n",
      "Iteration 3599000, loss=0.46942089292407047\n",
      "Nearest to trump: diamond, sovereign, 64, real-estate, suit, tap, always, approach-to,\n",
      "Nearest to apple: apple-s, google, microsoft, jobs, the-iphone, the-ipad, iphone, developers,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, biden, as-mr, aide, obama-and,\n",
      "Nearest to new-york-city: transit, city-s, county, city, mayor, police, nyt, the-mayor,\n",
      "Nearest to washington: washington-and, president-obama, chicago, in-washington, york, studies, albany, senators,\n",
      "Nearest to sport: the-sport, baseball, players, hockey, sports, horse, athletes, men-s,\n",
      "Nearest to rarely: often, typically, athletes, sometimes, have-never, few, be-usually, easier-to,\n",
      "Nearest to contrast: comparison, achieve, stark, which-mr, emotional, attitude, chic, charm,\n",
      "Nearest to democrat: senator, democratic, republican, a-democrat, a-republican, congressman, incumbent, representative,\n",
      "Nearest to somehow: scrap, survive, forgive, it-but, you-will, manage-to, arrangements, i-just,\n",
      "Nearest to anticipate: expect, economists, analysts, loom, next-year, it, stage, even,\n",
      "Nearest to the-lead: lead, citigroup, producer, albert, cause-of, producers, next-to, wells,\n",
      "Nearest to fuel: gas, fossil, emissions, gasoline, efficient, demand-for, nuclear, manufacture,\n",
      "Nearest to first-round: second-round, pick, round-of, team, round, lose-to, matchup, champion,\n",
      "Nearest to probably: think-i, we, wouldn-t, i-would, i, anyway, say-i, so-i,\n",
      "Nearest to for-years: ve-be, for-decades, fight, have-become, haven-t, long, lately, who-work,\n",
      "Iteration 3699000, loss=0.4697179526984695\n",
      "Nearest to trump: diamond, sovereign, suit, 64, real-estate, be-important, always, trick,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, developers, iphone,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, biden, aide, as-mr,\n",
      "Nearest to new-york-city: city-s, mayor, transit, county, city, police, nyt, residents,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, chicago, studies, lawmakers, center-on,\n",
      "Nearest to sport: the-sport, hockey, sports, baseball, athletes, men-s, players, horse,\n",
      "Nearest to rarely: often, typically, athletes, have-never, sometimes, productions, be-usually, or-even,\n",
      "Nearest to contrast: stark, emotional, which-mr, achieve, comparison, the-current, charm, modest,\n",
      "Nearest to democrat: senator, a-democrat, democratic, republican, congressman, the-republican, a-republican, representative,\n",
      "Nearest to somehow: scrap, manage-to, i-just, it-but, flexible, you-will, forgive, survive,\n",
      "Nearest to anticipate: expect, economists, analysts, next-year, loom, the-begin, it, stage,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producers, producer, previously, blumenthal, a-lead,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, gasoline, boost, efficient, manufacture,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, team, bears, matchup,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, that-may, i-would, guess, anyway,\n",
      "Nearest to for-years: ve-be, for-decades, have-become, fight, long, haven-t, who-work, lately,\n",
      "Iteration 3799000, loss=0.46945928624272365\n",
      "Nearest to trump: diamond, 64, sovereign, approach-to, real-estate, always, trick, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, developers, software,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, biden, as-mr, obama-s, obama-and,\n",
      "Nearest to new-york-city: county, city-s, mayor, city, transit, nyt, residents, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, chicago, lawmakers, studies, kenneth,\n",
      "Nearest to sport: the-sport, hockey, players, athletes, sports, baseball, men-s, horse,\n",
      "Nearest to rarely: typically, often, sometimes, such, have-never, athletes, pay-to, be-usually,\n",
      "Nearest to contrast: stark, comparison, charm, emotional, which-mr, modest, achieve, attitude,\n",
      "Nearest to democrat: senator, a-democrat, democratic, republican, the-republican, congressman, representative, support-of,\n",
      "Nearest to somehow: scrap, manage-to, universe, it-but, flexible, i-just, you-will, certainly,\n",
      "Nearest to anticipate: analysts, expect, economists, next-year, loom, it, the-begin, stage,\n",
      "Nearest to the-lead: lead, cause-of, citigroup, producers, previously, blumenthal, singer, portray,\n",
      "Nearest to fuel: gas, emissions, fossil, demand-for, efficient, manufacture, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, team, champion, draft,\n",
      "Nearest to probably: we, wouldn-t, i, think-i, anyway, i-would, guess, say-i,\n",
      "Nearest to for-years: ve-be, have-become, for-decades, fight, haven-t, the-obama, plague, who-work,\n",
      "Iteration 3899000, loss=0.46985952672362313\n",
      "Nearest to trump: diamond, sovereign, approach-to, always, heart, 64, real-estate, aesthetic,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, developers, the-ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, biden, as-mr, the-white-house,\n",
      "Nearest to new-york-city: city, mayor, city-s, county, nyt, transit, residents, chatter,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, studies, kenneth, obama-have,\n",
      "Nearest to sport: the-sport, hockey, athletes, players, baseball, men-s, horse, sports,\n",
      "Nearest to rarely: often, athletes, typically, such, acknowledge-that, sometimes, female, have-never,\n",
      "Nearest to contrast: stark, emotional, comparison, which-mr, decline-in, attitude, modest, charm,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, the-republican, congressman, representative, senate,\n",
      "Nearest to somehow: scrap, manage-to, universe, arrangements, you-will, dead, touch, i-just,\n",
      "Nearest to anticipate: expect, economists, analysts, next-year, stage, the-begin, it, acquisitions,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, wells, producers, polish, previously,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, manufacture, efficient, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, lose-to, round-of, the-2009, draft, champion, finals,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, anyway, so-i, that-may, i-would,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, fight, who-work, haven-t, long, plague,\n",
      "Iteration 3999000, loss=0.4701990739107131"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001))\n",
    "aux_loss=0\n",
    "cnt_save=cnt\n",
    "for cnt in range(cnt_save,epochs+cnt_save):\n",
    "    idx = np.random.randint(0, len(labels)-1,batch_size)\n",
    "    arr_1[:,] = word_target[idx]\n",
    "    arr_2[:,] = word_context[idx]\n",
    "    arr_3[:,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    aux_loss=aux_loss+loss/1000\n",
    "    write_log(callback, train_names, loss, cnt)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"\\rIteration {}, loss={}\".format(cnt, aux_loss),end=\"\")\n",
    "        aux_loss=0\n",
    "    if (cnt+1) % 100000 == 0:\n",
    "        print()\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples[0]=6761 #trump\n",
    "valid_examples[1]=1269 # apple\n",
    "valid_examples[2]=1991 #clinton\n",
    "valid_examples[3]=834 #new-york-city\n",
    "valid_examples[4]=597 #washington\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"washington\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest to trump: diamond, sovereign, approach-to, real-estate, always, 64, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, the-ipad, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, aide, as-mr, gibbs, obama-s,\n",
      "Nearest to new-york-city: city, chatter, county, city-s, the-brooklyn, transit, the-mayor, westchester,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, studies, in-washington, mutual, kenneth,\n",
      "Nearest to sport: the-sport, athletes, players, baseball, hockey, men-s, sports, horse,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, athletes, have-never, occasionally, acknowledge-that,\n",
      "Nearest to contrast: comparison, stark, achieve, charm, emotional, the-current, decline-in, modest,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, congressman, candidate,\n",
      "Nearest to somehow: scrap, manage-to, it-but, arrangements, universe, of-course, forgive, you-will,\n",
      "Nearest to anticipate: expect, economists, next-year, analysts, further, acquisitions, confirmation, loom,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, wells, singer, polish, six, portray,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, efficient, manufacture, nuclear, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, draft, lose-to, team, the-2009,\n",
      "Nearest to probably: we, think-i, i, wouldn-t, so-i, anyway, that-may, guess,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, have-become, long, fight, lately, plague,\n",
      "Iteration 4099000, loss=0.4715129273831848\n",
      "Nearest to trump: diamond, sovereign, approach-to, real-estate, always, 64, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, mobile, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, aide, obama, bush, as-mr, obama-and,\n",
      "Nearest to new-york-city: city, chatter, county, mayor, city-s, the-brooklyn, transit, westchester,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, studies, kenneth, mutual,\n",
      "Nearest to sport: the-sport, athletes, players, baseball, men-s, hockey, sports, horse,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, have-never, athletes, occasionally, see,\n",
      "Nearest to contrast: comparison, stark, achieve, charm, emotional, attitude, which-mr, the-current,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, congressman, senate,\n",
      "Nearest to somehow: scrap, manage-to, it-but, universe, arrangements, forgive, dead, you-will,\n",
      "Nearest to anticipate: expect, economists, next-year, analysts, further, it, confirmation, loom,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, producers, polish, blumenthal, six,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, nuclear, manufacture, dioxide,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, the-2009, draft,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, anyway, that-may, guess,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, fight, have-become, years-of, long, the-obama,\n",
      "Iteration 4199000, loss=0.46843408066034353\n",
      "Nearest to trump: diamond, sovereign, approach-to, 64, real-estate, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, iphone, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, aide, obama, obama-and, as-mr, bush,\n",
      "Nearest to new-york-city: city, chatter, county, city-s, mayor, the-brooklyn, transit, westchester,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, studies, kenneth, mutual,\n",
      "Nearest to sport: the-sport, athletes, players, baseball, hockey, sports, men-s, horse,\n",
      "Nearest to rarely: typically, often, sometimes, have-never, athletes, be-usually, such, see,\n",
      "Nearest to contrast: comparison, achieve, stark, charm, emotional, decline-in, which-mr, the-current,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, congressman, the-republican, a-republican,\n",
      "Nearest to somehow: scrap, manage-to, it-but, universe, arrangements, dead, forgive, of-course,\n",
      "Nearest to anticipate: expect, next-year, economists, analysts, loom, further, it, the-possibility,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, six, polish, producers, wells,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, nuclear, coal,\n",
      "Nearest to first-round: second-round, pick, round-of, draft, lose-to, round, team, the-2009,\n",
      "Nearest to probably: think-i, wouldn-t, i, we, so-i, anyway, that-may, say-i,\n",
      "Nearest to for-years: ve-be, for-decades, haven-t, fight, long, lately, have-become, years-of,\n",
      "Iteration 4299000, loss=0.46961716291308464\n",
      "Nearest to trump: diamond, sovereign, 64, approach-to, always, real-estate, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, ipad, iphone,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, aide, obama, as-mr, obama-and, bush,\n",
      "Nearest to new-york-city: city, chatter, county, city-s, transit, mayor, the-mayor, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, chicago, york, in-washington, kenneth, studies, mutual,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, men-s, hockey, sports, horse,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, have-never, athletes, occasionally, see,\n",
      "Nearest to contrast: comparison, achieve, stark, charm, attitude, emotional, which-mr, modest,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, congressman, the-republican, a-republican,\n",
      "Nearest to somehow: scrap, manage-to, it-but, universe, arrangements, dead, forgive, you-will,\n",
      "Nearest to anticipate: expect, next-year, economists, analysts, further, loom, the-possibility, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, polish, six, previously, producers,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, efficient, manufacture, nuclear, dioxide,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, the-2009, draft, team,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, anyway, that-may, guess,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, fight, the-obama, who-work, plague, have-become,\n",
      "Iteration 4399000, loss=0.4714591118991371\n",
      "Nearest to trump: diamond, sovereign, real-estate, approach-to, 64, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, aide, obama, bush, obama-and, as-mr,\n",
      "Nearest to new-york-city: city, chatter, county, mayor, city-s, the-brooklyn, transit, the-mayor,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, chicago, york, kenneth, studies, mutual,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, men-s, hockey, sports, horse,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, have-never, athletes, occasionally, see,\n",
      "Nearest to contrast: comparison, achieve, charm, attitude, stark, the-current, modest, which-mr,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, congressman, the-republican, a-republican,\n",
      "Nearest to somehow: scrap, manage-to, it-but, universe, dead, you-will, survive, and-you,\n",
      "Nearest to anticipate: next-year, expect, analysts, economists, further, loom, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, six, polish, previously, blumenthal,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, nuclear, manufacture, dioxide,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, draft, team, the-2009,\n",
      "Nearest to probably: we, think-i, i, wouldn-t, so-i, that-may, anyway, i-would,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, haven-t, the-obama, fight, lately, who-work,\n",
      "Iteration 4499000, loss=0.4696015019714827\n",
      "Nearest to trump: diamond, sovereign, real-estate, approach-to, 64, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, iphone, the-ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, aide, obama, bush, as-mr, obama-and,\n",
      "Nearest to new-york-city: city, chatter, county, mayor, city-s, transit, the-mayor, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, chicago, in-washington, york, kenneth, mutual, studies,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, men-s, hockey, sports, horse,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, athletes, have-never, occasionally, acknowledge-that,\n",
      "Nearest to contrast: achieve, comparison, stark, attitude, charm, modest, which-mr, the-current,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, congressman, a-republican,\n",
      "Nearest to somehow: scrap, manage-to, universe, survive, and-yet, it-but, of-course, arrangements,\n",
      "Nearest to anticipate: next-year, expect, economists, analysts, loom, further, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, polish, singer, previously, six, producers,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, efficient, nuclear, manufacture, coal,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, draft, the-2009, team,\n",
      "Nearest to probably: we, think-i, i, wouldn-t, so-i, i-would, that-may, anyway,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, have-become, who-work, the-obama, fight, years-of,\n",
      "Iteration 4599000, loss=0.4701372741758825\n",
      "Nearest to trump: diamond, sovereign, real-estate, 64, approach-to, heart, client, trick,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, the-ipad, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, aide, gibbs, obama-s,\n",
      "Nearest to new-york-city: city, chatter, county, city-s, mayor, transit, the-mayor, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, kenneth, studies, the-editor,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, hockey, men-s, horse, sports,\n",
      "Nearest to rarely: sometimes, typically, often, have-never, be-usually, athletes, occasionally, such,\n",
      "Nearest to contrast: achieve, comparison, the-current, stark, attitude, charm, modest, which-mr,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, congressman, democrat-of, the-republican,\n",
      "Nearest to somehow: scrap, manage-to, it-but, universe, survive, and-yet, arrangements, of-course,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, further, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, singer, previously, producers, cause-of, polish, 29,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, nuclear, gasoline,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, team, draft, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, i-would, anyway, that-may,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, have-become, who-work, long, lately, fight,\n",
      "Iteration 4699000, loss=0.47073464807868065\n",
      "Nearest to trump: diamond, sovereign, real-estate, approach-to, 64, always, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, mobile, the-ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, aide, gibbs, obama-s,\n",
      "Nearest to new-york-city: city, chatter, county, city-s, mayor, transit, the-brooklyn, the-mayor,\n",
      "Nearest to washington: washington-and, president-obama, chicago, in-washington, york, kenneth, studies, mutual,\n",
      "Nearest to sport: the-sport, athletes, hockey, players, baseball, men-s, sports, horse,\n",
      "Nearest to rarely: often, sometimes, typically, be-usually, have-never, athletes, occasionally, acknowledge-that,\n",
      "Nearest to contrast: achieve, comparison, charm, attitude, stark, the-current, modest, which-mr,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, democrat-of, incumbent, the-republican,\n",
      "Nearest to somehow: scrap, manage-to, and-yet, universe, survive, arrangements, it-but, of-course,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, further, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, singer, previously, 29, cause-of, producers, polish,\n",
      "Nearest to fuel: gas, fossil, emissions, demand-for, efficient, manufacture, nuclear, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, team, draft, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, that-may, i-would, anyway,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, haven-t, fight, the-obama, long, plague,\n",
      "Iteration 4799000, loss=0.4684747675657269\n",
      "Nearest to trump: diamond, sovereign, real-estate, approach-to, 64, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, the-ipad, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, as-mr, obama, obama-and, aide, gibbs,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, the-mayor, the-brooklyn, mayor,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, chicago, kenneth, studies, mutual,\n",
      "Nearest to sport: the-sport, athletes, hockey, baseball, players, men-s, horse, sports,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, have-never, athletes, such, occasionally,\n",
      "Nearest to contrast: comparison, achieve, charm, the-current, stark, attitude, modest, style,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, congressman, incumbent, the-republican,\n",
      "Nearest to somehow: scrap, manage-to, universe, survive, and-yet, of-course, arrangements, it-but,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, further, loom, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, producers, singer, previously, 29, cause-of, six,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, gasoline, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, anyway, i-would, so-i, that-may,\n",
      "Nearest to for-years: for-decades, ve-be, haven-t, have-become, fight, lately, long, years-of,\n",
      "Iteration 4899000, loss=0.46855841898918066\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, always, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, the-ipad, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, obama-and, aide, gibbs,\n",
      "Nearest to new-york-city: city, city-s, chatter, county, westchester, the-mayor, the-brooklyn, mayor,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, studies, kenneth, mutual,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, hockey, men-s, sports, horse,\n",
      "Nearest to rarely: sometimes, often, typically, have-never, be-usually, athletes, such, occasionally,\n",
      "Nearest to contrast: achieve, stark, comparison, charm, attitude, which-mr, the-current, modest,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, incumbent, democrat-of, the-republican,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, of-course, and-yet, while, i-just,\n",
      "Nearest to anticipate: next-year, expect, analysts, economists, loom, further, it, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, singer, producers, cause-of, 29, six, previously,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, gasoline, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, i-would, anyway, that-may,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, the-obama, plague, fight, haven-t, lately,\n",
      "Iteration 4999000, loss=0.46918389359116536\n",
      "Nearest to trump: diamond, sovereign, real-estate, 64, approach-to, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, ipad, the-ipad, mobile,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, aide, obama-and, obama-s,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, mayor, the-brooklyn, the-mayor,\n",
      "Nearest to washington: washington-and, president-obama, in-washington, chicago, york, kenneth, studies, mutual,\n",
      "Nearest to sport: the-sport, athletes, baseball, horse, players, men-s, hockey, sports,\n",
      "Nearest to rarely: sometimes, often, typically, be-usually, have-never, athletes, occasionally, such,\n",
      "Nearest to contrast: comparison, achieve, charm, the-current, stark, modest, attitude, decline-in,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, incumbent, congressman, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, of-course, and-yet, you-will, arrangements,\n",
      "Nearest to anticipate: next-year, expect, analysts, economists, further, loom, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producers, singer, polish, six, 29,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, so-i, anyway, i-would, that-may,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, plague, haven-t, fight, the-obama, lately,\n",
      "Iteration 5099000, loss=0.46924426010251075\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, client, heart, always,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, iphone, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, obama-and, aide, obama-s,\n",
      "Nearest to new-york-city: city, city-s, chatter, county, westchester, the-mayor, nyt, transit,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, kenneth, studies, trade,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, sports, men-s,\n",
      "Nearest to rarely: sometimes, typically, often, be-usually, athletes, have-never, occasionally, meet,\n",
      "Nearest to contrast: charm, achieve, comparison, stark, the-current, attitude, modest, which-mr,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, incumbent, congressman, the-republican,\n",
      "Nearest to somehow: manage-to, scrap, universe, survive, of-course, and-yet, arrangements, it-but,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, further, loom, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, singer, producers, cause-of, 29, six, polish,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, round, lose-to, draft, team, champion,\n",
      "Nearest to probably: we, wouldn-t, i, think-i, anyway, say-i, i-would, so-i,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, the-obama, lately, fight, haven-t, plague,\n",
      "Iteration 5199000, loss=0.4687712054550647\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, heart, client, trick,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, iphone, the-ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, aide, obama-and, gibbs,\n",
      "Nearest to new-york-city: city, city-s, chatter, county, westchester, the-brooklyn, the-mayor, mayor,\n",
      "Nearest to washington: washington-and, president-obama, york, studies, chicago, in-washington, kenneth, trade,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, sports, men-s,\n",
      "Nearest to rarely: often, typically, sometimes, be-usually, athletes, have-never, see, occasionally,\n",
      "Nearest to contrast: achieve, charm, comparison, stark, the-current, attitude, modest, which-mr,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, incumbent, the-republican, congressman,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, and-yet, arrangements, of-course, it-but,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, further, confirmation, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, six, producers, wells, 29,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, gasoline, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, the-2009,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, so-i, i-would, anyway, say-i,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, the-obama, fight, lately, haven-t, plague,\n",
      "Iteration 5299000, loss=0.46895547693967854\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, client, heart, always,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, mobile, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, aide, as-mr, obama-and, obama-s,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, mayor, the-mayor, nyt,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, studies, kenneth, the-editor,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, hockey, horse, sports, men-s,\n",
      "Nearest to rarely: often, typically, sometimes, be-usually, athletes, have-never, occasionally, see,\n",
      "Nearest to contrast: comparison, achieve, charm, the-current, stark, modest, attitude, decline-in,\n",
      "Nearest to democrat: senator, republican, a-democrat, democratic, representative, the-republican, incumbent, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, arrangements, and-yet, of-course, it-but,\n",
      "Nearest to anticipate: next-year, expect, analysts, economists, loom, further, it, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producers, polish, singer, six, 29,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, boost, gasoline,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, so-i, i-would, anyway, say-i,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, plague, the-obama, haven-t, lately, fight,\n",
      "Iteration 5399000, loss=0.46919810554385216\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, heart, maximum, trick,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, as-mr, gibbs, obama-s, obama-and,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, nyt, the-mayor, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, york, chicago, in-washington, the-editor, kenneth, studies,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, sports, men-s,\n",
      "Nearest to rarely: sometimes, typically, often, be-usually, athletes, have-never, such, see,\n",
      "Nearest to contrast: comparison, achieve, charm, the-current, stark, which-mr, attitude, modest,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, democrat-of, incumbent,\n",
      "Nearest to somehow: manage-to, scrap, universe, survive, arrangements, it-but, of-course, and-yet,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, confirmation, further, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, producers, six, polish, portray,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, draft, team, champion,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, anyway, i-would, so-i, say-i,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, plague, haven-t, fight, the-obama, lately,\n",
      "Iteration 5499000, loss=0.46872944921255105\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, heart, always, trick,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, aide, obama-and, as-mr, obama-s,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, york-state, mayor, nyt,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, studies, the-editor, chicago, kenneth,\n",
      "Nearest to sport: the-sport, athletes, baseball, hockey, players, horse, men-s, sports,\n",
      "Nearest to rarely: typically, sometimes, often, be-usually, have-never, athletes, occasionally, see,\n",
      "Nearest to contrast: comparison, charm, achieve, stark, the-current, which-mr, attitude, emotional,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, democrat-of, incumbent,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, arrangements, it-but, of-course, and-yet,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, further, it, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, producers, portray, six, wells,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, nuclear, gasoline,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, i-would, anyway, so-i, say-i,\n",
      "Nearest to for-years: for-decades, ve-be, have-become, plague, haven-t, the-obama, lately, fight,\n",
      "Iteration 5599000, loss=0.4681257036626336\n",
      "Nearest to trump: diamond, real-estate, sovereign, 64, approach-to, always, heart, client,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, aide, as-mr, gibbs,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, nyt, mayor, york-state,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, the-editor, chicago, studies, kenneth,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, men-s, sports,\n",
      "Nearest to rarely: sometimes, typically, often, have-never, be-usually, athletes, such, occasionally,\n",
      "Nearest to contrast: comparison, achieve, charm, attitude, stark, the-current, modest, reflect,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, incumbent, the-republican, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, and-yet, arrangements, of-course, it-but,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, confirmation, further, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, singer, producers, wells, portray, previously,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, gasoline, boost,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, team, draft, round, champion,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, anyway, i-would, so-i, say-i,\n",
      "Nearest to for-years: for-decades, have-become, plague, ve-be, the-obama, lately, fight, decades,\n",
      "Iteration 5699000, loss=0.4686246171891687\n",
      "Nearest to trump: diamond, real-estate, sovereign, approach-to, 64, always, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, aide, as-mr, gibbs,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, mayor, nyt, the-brooklyn,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, kenneth, chicago, the-editor, studies,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, sports, men-s,\n",
      "Nearest to rarely: often, typically, sometimes, have-never, be-usually, athletes, see, female,\n",
      "Nearest to contrast: comparison, achieve, charm, stark, attitude, the-current, modest, create-a,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, incumbent, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, survive, universe, and-yet, arrangements, of-course, it-but,\n",
      "Nearest to anticipate: next-year, expect, analysts, economists, loom, further, it, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, producers, cause-of, portray, previously, singer, blumenthal,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, gasoline, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, team, draft, round, champion,\n",
      "Nearest to probably: we, think-i, wouldn-t, i, i-would, anyway, say-i, so-i,\n",
      "Nearest to for-years: for-decades, have-become, ve-be, plague, the-obama, lately, fight, decades,\n",
      "Iteration 5799000, loss=0.4705006649494175\n",
      "Nearest to trump: diamond, sovereign, real-estate, 64, approach-to, always, client, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, the-ipad, mobile, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, aide, as-mr, gibbs,\n",
      "Nearest to new-york-city: city, city-s, county, chatter, westchester, mayor, nyt, york-state,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, the-editor, chicago, studies, kenneth,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, horse, hockey, men-s, sports,\n",
      "Nearest to rarely: often, sometimes, typically, be-usually, have-never, athletes, female, such,\n",
      "Nearest to contrast: achieve, comparison, charm, the-current, attitude, stark, modest, emotional,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, incumbent, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, universe, survive, and-yet, arrangements, deeply, it-but,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, further, loom, it, confirmation,\n",
      "Nearest to the-lead: lead, citigroup, producers, cause-of, blumenthal, previously, portray, singer,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, manufacture, efficient, nuclear, gasoline,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, draft, team, round, champion,\n",
      "Nearest to probably: we, wouldn-t, think-i, i, anyway, i-would, say-i, so-i,\n",
      "Nearest to for-years: for-decades, have-become, fight, plague, the-obama, ve-be, lately, long,\n",
      "Iteration 5899000, loss=0.4675949787199503\n",
      "Nearest to trump: diamond, sovereign, real-estate, 64, approach-to, client, always, heart,\n",
      "Nearest to apple: apple-s, microsoft, google, jobs, the-iphone, mobile, the-ipad, ipad,\n",
      "Nearest to clinton: hillary, mrs, secretary-of, obama, obama-and, as-mr, aide, obama-s,\n",
      "Nearest to new-york-city: city, county, city-s, westchester, chatter, nyt, york-state, mayor,\n",
      "Nearest to washington: washington-and, president-obama, york, in-washington, studies, chicago, the-editor, trade,\n",
      "Nearest to sport: the-sport, athletes, baseball, players, hockey, horse, sports, men-s,\n",
      "Nearest to rarely: often, typically, sometimes, be-usually, have-never, athletes, see, occasionally,\n",
      "Nearest to contrast: comparison, achieve, charm, stark, attitude, the-current, modest, emotional,\n",
      "Nearest to democrat: senator, a-democrat, republican, democratic, representative, the-republican, incumbent, democrat-of,\n",
      "Nearest to somehow: manage-to, scrap, arrangements, universe, survive, and-yet, it-but, of-course,\n",
      "Nearest to anticipate: expect, next-year, analysts, economists, loom, further, progress, it,\n",
      "Nearest to the-lead: lead, citigroup, cause-of, producers, portray, blumenthal, singer, previously,\n",
      "Nearest to fuel: gas, fossil, demand-for, emissions, efficient, manufacture, gasoline, nuclear,\n",
      "Nearest to first-round: second-round, pick, round-of, lose-to, round, team, draft, champion,\n",
      "Nearest to probably: we, wouldn-t, i, think-i, i-would, say-i, anyway, so-i,\n",
      "Nearest to for-years: for-decades, have-become, ve-be, plague, the-obama, fight, lately, long,\n",
      "Iteration 5999000, loss=0.46960656934976613"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00001))\n",
    "aux_loss=0\n",
    "cnt_save=cnt\n",
    "for cnt in range(cnt_save,epochs+cnt_save):\n",
    "    idx = np.random.randint(0, len(labels)-1,batch_size)\n",
    "    arr_1[:,] = word_target[idx]\n",
    "    arr_2[:,] = word_context[idx]\n",
    "    arr_3[:,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    aux_loss=aux_loss+loss/1000\n",
    "    write_log(callback, train_names, loss, cnt)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"\\rIteration {}, loss={}\".format(cnt, aux_loss),end=\"\")\n",
    "        aux_loss=0\n",
    "    if (cnt+1) % 100000 == 0:\n",
    "        print()\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"2010.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('2010.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_model.save(\"2010_val.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del validation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del couples\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-939330b76721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=helper.get_vocabulary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
